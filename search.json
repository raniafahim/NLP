[
  {
    "objectID": "5_ressources.html",
    "href": "5_ressources.html",
    "title": "Pour aller plus loin",
    "section": "",
    "text": "Quelques ressources pour aller plus loin :",
    "crumbs": [
      "Ressources"
    ]
  },
  {
    "objectID": "5_ressources.html#dautres-ressources-pour-aller-plus-loin",
    "href": "5_ressources.html#dautres-ressources-pour-aller-plus-loin",
    "title": "Pour aller plus loin",
    "section": "",
    "text": "Quelques ressources pour aller plus loin :",
    "crumbs": [
      "Ressources"
    ]
  },
  {
    "objectID": "4_embedding.html",
    "href": "4_embedding.html",
    "title": "Du mot au vecteur",
    "section": "",
    "text": "Un embedding est un espace vectoriel permettant dâ€™attribuer une reprÃ©sentation numÃ©rique Ã  un token. GrÃ¢ce Ã  ces reprÃ©sentations, la machine peut effectuer des opÃ©rations arithmÃ©tiques pour manipuler le langage et dÃ©terminer si des mots sont proches ou non sur le plan sÃ©mantique.\n\n\n\nAvec un vocabulaire de tokens de taille ( n ), on peut crÃ©er un espace vectoriel de dimension ( n ), oÃ¹ chaque token est reprÃ©sentÃ© par un vecteur unitaire canonique (one-hot encoding). Cependant, cette approche prÃ©sente plusieurs inconvÃ©nients :\n1. Lâ€™espace vectoriel devient trÃ¨s grand lorsque ( n ) est important.\n2. Les reprÃ©sentations sont orthogonales, ce qui signifie quâ€™elles ne capturent aucune relation sÃ©mantique entre les mots.\n\n\n\nEn sâ€™appuyant sur lâ€™hypothÃ¨se distributionnelle et les rÃ©seaux de neurones, les modÃ¨les neuronaux Bag of Words (CBOW et Skip-Gram) permettent dâ€™apprendre des reprÃ©sentations vectorielles compressÃ©es tout en conservant les proximitÃ©s et distances sÃ©mantiques entre les tokens.",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#embedding-passer-du-token-au-vecteur-numÃ©rique",
    "href": "4_embedding.html#embedding-passer-du-token-au-vecteur-numÃ©rique",
    "title": "Du mot au vecteur",
    "section": "",
    "text": "Un embedding est un espace vectoriel permettant dâ€™attribuer une reprÃ©sentation numÃ©rique Ã  un token. GrÃ¢ce Ã  ces reprÃ©sentations, la machine peut effectuer des opÃ©rations arithmÃ©tiques pour manipuler le langage et dÃ©terminer si des mots sont proches ou non sur le plan sÃ©mantique.\n\n\n\nAvec un vocabulaire de tokens de taille ( n ), on peut crÃ©er un espace vectoriel de dimension ( n ), oÃ¹ chaque token est reprÃ©sentÃ© par un vecteur unitaire canonique (one-hot encoding). Cependant, cette approche prÃ©sente plusieurs inconvÃ©nients :\n1. Lâ€™espace vectoriel devient trÃ¨s grand lorsque ( n ) est important.\n2. Les reprÃ©sentations sont orthogonales, ce qui signifie quâ€™elles ne capturent aucune relation sÃ©mantique entre les mots.\n\n\n\nEn sâ€™appuyant sur lâ€™hypothÃ¨se distributionnelle et les rÃ©seaux de neurones, les modÃ¨les neuronaux Bag of Words (CBOW et Skip-Gram) permettent dâ€™apprendre des reprÃ©sentations vectorielles compressÃ©es tout en conservant les proximitÃ©s et distances sÃ©mantiques entre les tokens.",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#les-premiers-algorithmes-sÃ©mantiques",
    "href": "4_embedding.html#les-premiers-algorithmes-sÃ©mantiques",
    "title": "Du mot au vecteur",
    "section": "Les premiers algorithmes sÃ©mantiques",
    "text": "Les premiers algorithmes sÃ©mantiques\n\nğŸ”¹ Lâ€™algorithme CBOW\nLâ€™algorithme Continuous Bag of Words (CBOW) repose sur un rÃ©seau de neurones composÃ© de deux Ã©tapes :\n1. Encodage : une couche dâ€™entrÃ©e qui compresse les mots du contexte dans un vecteur de taille rÃ©duite.\n2. DÃ©codage : une couche de sortie qui prend ce vecteur rÃ©duit et prÃ©dit le mot cible.\nLâ€™idÃ©e est dâ€™entraÃ®ner le rÃ©seau Ã  prÃ©dire un mot Ã  partir de son contexte (les mots qui lâ€™entourent).\nExemple avec un contexte de 4 mots (les deux mots avant et aprÃ¨s) :\n- Phrase : â€œLe chat dort sur le canapÃ©.â€\n- Contexte : [\"Le\", \"dort\", \"sur\", \"le\"]\n- Mot Ã  prÃ©dire : \"chat\"\n- Base dâ€™entraÃ®nement : ((â€œLeâ€, â€œdortâ€, â€œsurâ€, â€œleâ€), â€œchatâ€)\nEn parcourant un corpus, lâ€™algorithme apprend ces associations de maniÃ¨re probabiliste. Une fois lâ€™entraÃ®nement terminÃ©, on rÃ©cupÃ¨re la reprÃ©sentation vectorielle intermÃ©diaire pour associer chaque mot Ã  son embedding.\n\n\nğŸ”¹ Lâ€™algorithme Skip-Gram\nLâ€™architecture du rÃ©seau est similaire Ã  CBOW, mais avec une approche inversÃ©e :\n- PlutÃ´t que de prÃ©dire un mot Ã  partir de son contexte, Skip-Gram prÃ©dit les mots du contexte Ã  partir dâ€™un mot donnÃ©.\n- On gÃ©nÃ¨re des paires (mot, mot_contexte) en fonction de la fenÃªtre de contexte choisie.\nExemple :\n- Phrase : â€œLe chat dort sur le canapÃ©.â€\n- Mot donnÃ© : \"chat\"\n- Contexte Ã  prÃ©dire : [\"Le\", \"dort\", \"sur\", \"le\"]\n- Base dâ€™entraÃ®nement : (â€œLeâ€,â€œchatâ€) ; (â€œdortâ€,â€œchatâ€) ; (â€œsurâ€,â€œchatâ€) ; (â€œleâ€,â€œchatâ€)\nUne fois la distribution apprise, on dÃ©branche la deuxiÃ¨me couche et on utilise la rÃ©prÃ©sentation vectorielle de la couche cachÃ©e.\n\n\nğŸ”¹ Lâ€™algorithme GloVe\nLâ€™algorithme GloVe (Global Vectors for Word Representation) adopte une approche diffÃ©rente :\n- PlutÃ´t que dâ€™analyser des contextes locaux (comme CBOW et Skip-Gram), GloVe apprend des cooccurrences de mots Ã  partir dâ€™un large corpus.\n- Il construit une matrice de cooccurrence indiquant Ã  quelle frÃ©quence deux mots apparaissent ensemble dans les mÃªmes phrases.\n- Ensuite, un facteur de dÃ©composition est utilisÃ© pour gÃ©nÃ©rer des reprÃ©sentations vectorielles capturant les relations sÃ©mantiques.\nGloVe est particuliÃ¨rement efficace pour reprÃ©senter des mots ayant des relations sÃ©mantiques globales, comme :\n- Roi â€“ Reine,\n- France â€“ Paris,\n- Banque â€“ Argent.\n\n\nğŸ”¹ Lâ€™algorithme FastText\nFastText est une amÃ©lioration de CBOW et Skip-Gram :\n- Au lieu de reprÃ©senter un mot en entier, il le dÃ©compose en sous-mots (n-grams).\n- Cela permet de mieux gÃ©rer les mots rares ou inconnus en gÃ©nÃ©rant des embeddings dynamiques.\nExemple :\n- Le mot â€œapprentissageâ€ peut Ãªtre dÃ©composÃ© en [\"app\", \"pren\", \"tiss\", \"age\"].\n- Si un mot jamais vu auparavant est rencontrÃ©, son embedding peut Ãªtre infÃ©rÃ© Ã  partir de ses sous-mots.\nFastText est particuliÃ¨rement utile pour les langues morphologiquement riches (comme lâ€™allemand ou le turc) et pour gÃ©rer les fautes dâ€™orthographe ou les variantes linguistiques.",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#les-embeddings-contextuels",
    "href": "4_embedding.html#les-embeddings-contextuels",
    "title": "Du mot au vecteur",
    "section": "Les embeddings contextuels",
    "text": "Les embeddings contextuels\nLe principal problÃ¨me des embeddings classiques est que leur reprÃ©sentation est fixe : un mÃªme token possÃ¨de toujours le mÃªme vecteur numÃ©rique, quel que soit le contexte. Or, dans de nombreuses langues, notamment en franÃ§ais, un mÃªme mot peut avoir plusieurs significations selon son usage. De plus, des notions diffÃ©rentes peuvent sâ€™Ã©crire de la mÃªme faÃ§on (homonymes).\nPour dÃ©sambiguÃ¯ser ces cas, il est essentiel de prendre en compte le contexte dans lequel un mot apparaÃ®t afin de dÃ©terminer son sens prÃ©cis.\nLes embeddings contextuels permettent dâ€™adapter la reprÃ©sentation numÃ©rique dâ€™un mot en fonction de son contexte. Contrairement aux approches statiques, ils tiennent compte de la position et de lâ€™interaction entre les mots dans une phrase.\nExemple :\n- â€œLa baleine nage dans lâ€™ocÃ©an.â€ ğŸ‹ â†’ baleine (animal)\n- â€œJâ€™ai un mal de baleine aprÃ¨s ce repas.â€ ğŸ¤• â†’ baleine (expression signifiant une douleur intense)\nLes modÃ¨les basÃ©s sur les transformers (ex. BERT, GPT, T5) sont capables dâ€™encoder ces diffÃ©rences en gÃ©nÃ©rant un vecteur unique pour chaque occurrence dâ€™un mot selon son contexte. Ainsi, un mot aura un embedding spÃ©cifique qui varie en fonction des mots qui lâ€™entourent, tout en conservant les relations sÃ©mantiques apprises (synonymie, antonymie, etc.).",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#travaux-pratiques",
    "href": "4_embedding.html#travaux-pratiques",
    "title": "Du mot au vecteur",
    "section": "Travaux pratiques",
    "text": "Travaux pratiques",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "Introduction au NLP",
    "section": "",
    "text": "Il existe plusieurs dÃ©finitions pour dÃ©finir le concept de langage naturel Lien Dictonnary .\n\nUn ensemble de mots et les systÃ¨mes de leur utilisation communs Ã  un peuple appartenant Ã  la mÃªme communautÃ© ou nation, Ã  la mÃªme zone gÃ©ographique ou Ã  la mÃªme tradition culturelle.\nLa communication par la voix de maniÃ¨re spÃ©cifiquement humaine, en utilisant des sons arbitraires de faÃ§on conventionnelle avec des significations conventionnelles.\nLe systÃ¨me de signes ou de symboles linguistiques considÃ©rÃ© dans lâ€™abstrait (opposÃ© Ã  la parole).\nTout ensemble ou systÃ¨me de tels symboles utilisÃ©s de maniÃ¨re plus ou moins uniforme par un certain nombre de personnes, leur permettant ainsi de communiquer intelligiblement entre elles.\nTout systÃ¨me de symboles formalisÃ©s, signes, sons, gestes, ou autres, utilisÃ© ou conÃ§u comme un moyen de communiquer la pensÃ©e, lâ€™Ã©motion, etc. : le langage des mathÃ©matiques ; la langue des signes. Les moyens de communication utilisÃ©s par les animaux.\n\nAinsi, on peut voir le traitement du langage naturel comme un ensemble de recherche et de dÃ©veloppement visant Ã  modÃ©liser et Ã  reproduire, Ã  lâ€™aide de machines, la capacitÃ© humaine Ã  produire et Ã  comprendre des Ã©noncÃ©s linguistiques Ã  des fins de communication. Le langage est ainsi un concept complexe et ambigu. Son interprÃ©tation sâ€™accompagne de nombreuses connaissances implicites (expressions, mÃ©taphores, mÃ©tonymiesâ€¦) et de bon sens, ce qui est trÃ¨s compliquÃ© Ã  retranscrire Ã  un ordinateur.\nExemples : Jâ€™ai lu un article sur le droit des femmes dans le journal. Jâ€™ai lu un article sur le droit des femmes dans le mÃ©tro.",
    "crumbs": [
      "Introduction au NLP"
    ]
  },
  {
    "objectID": "1_intro.html#quest-ce-que-le-traitement-du-langage-naturel",
    "href": "1_intro.html#quest-ce-que-le-traitement-du-langage-naturel",
    "title": "Introduction au NLP",
    "section": "",
    "text": "Il existe plusieurs dÃ©finitions pour dÃ©finir le concept de langage naturel Lien Dictonnary .\n\nUn ensemble de mots et les systÃ¨mes de leur utilisation communs Ã  un peuple appartenant Ã  la mÃªme communautÃ© ou nation, Ã  la mÃªme zone gÃ©ographique ou Ã  la mÃªme tradition culturelle.\nLa communication par la voix de maniÃ¨re spÃ©cifiquement humaine, en utilisant des sons arbitraires de faÃ§on conventionnelle avec des significations conventionnelles.\nLe systÃ¨me de signes ou de symboles linguistiques considÃ©rÃ© dans lâ€™abstrait (opposÃ© Ã  la parole).\nTout ensemble ou systÃ¨me de tels symboles utilisÃ©s de maniÃ¨re plus ou moins uniforme par un certain nombre de personnes, leur permettant ainsi de communiquer intelligiblement entre elles.\nTout systÃ¨me de symboles formalisÃ©s, signes, sons, gestes, ou autres, utilisÃ© ou conÃ§u comme un moyen de communiquer la pensÃ©e, lâ€™Ã©motion, etc. : le langage des mathÃ©matiques ; la langue des signes. Les moyens de communication utilisÃ©s par les animaux.\n\nAinsi, on peut voir le traitement du langage naturel comme un ensemble de recherche et de dÃ©veloppement visant Ã  modÃ©liser et Ã  reproduire, Ã  lâ€™aide de machines, la capacitÃ© humaine Ã  produire et Ã  comprendre des Ã©noncÃ©s linguistiques Ã  des fins de communication. Le langage est ainsi un concept complexe et ambigu. Son interprÃ©tation sâ€™accompagne de nombreuses connaissances implicites (expressions, mÃ©taphores, mÃ©tonymiesâ€¦) et de bon sens, ce qui est trÃ¨s compliquÃ© Ã  retranscrire Ã  un ordinateur.\nExemples : Jâ€™ai lu un article sur le droit des femmes dans le journal. Jâ€™ai lu un article sur le droit des femmes dans le mÃ©tro.",
    "crumbs": [
      "Introduction au NLP"
    ]
  },
  {
    "objectID": "2_tokenization.html",
    "href": "2_tokenization.html",
    "title": "Tokenization : premiÃ¨re Ã©tape du NLP",
    "section": "",
    "text": "La tokenisation est le processus de dÃ©coupage dâ€™un long Ã©lÃ©ment en plusieurs petits Ã©lÃ©ments. Elle est essentielle en NLP (traitement du langage naturel) afin de fournir progressivement des fragments dâ€™information Ã  la machine. Quel que soit sa longueur, un texte doit Ãªtre segmentÃ© en petits morceaux pour Ãªtre traitÃ© sÃ©quentiellement par lâ€™algorithme.\nLa tokenisation constitue la base du NLP et influence fortement les rÃ©sultats en fonction des choix effectuÃ©s. Traditionnellement, elle sâ€™effectue au niveau des mots ou des sous-mots : une phrase entiÃ¨re est trop volumineuse pour Ãªtre comprise par la machine, tandis que les caractÃ¨res pris individuellement sont trop atomiques pour les langues latines et cyrilliques.\nExemples de tokenisation :\n\nDÃ©coupage en tokens mots :\nâ€œLa tokenisation en NLP est primordiale.â€ â†’ \"La\", \"tokenisation\", \"en\", \"NLP\", \"est\", \"primordiale\"\n\nDÃ©coupage en tokens sous-mots :\nâ€œLa tokenisation en NLP est primordiale.â€ â†’ \"La\", \"token\", \"isation\", \"en\", \"NLP\", \"est\", \"prim\", \"ordiale\"\n\n\n\n\n\n\nUne fois la tokenisation rÃ©alisÃ©e, on peut analyser les occurrences des tokens et observer des rÃ©gularitÃ©s statistiques, telles que :\n- Les mots les plus frÃ©quents (top mots).\n- Les mots qui apparaissent souvent ensemble, formant des expressions courantes.\n- Les mots qui reviennent dans des contextes similaires.\nEn NLP, on formule lâ€™hypothÃ¨se distributionnelle suivante : les mots qui apparaissent souvent dans des contextes similaires sont proches et a priori ont de fortes chances dâ€™avoir le mÃªme sens.\n\n\n\nLa machine ne comprend que des suites de 0 et 1. Il est donc nÃ©cessaire de convertir chaque token en une reprÃ©sentation numÃ©rique. Pour ce faire, on lui attribue un vecteur numÃ©rique de taille n (suffisamment grand, mais pas excessif).\nUne premiÃ¨re approche, appelÃ©e one-hot encoding, consiste Ã  associer Ã  chaque token un vecteur canonique ( e_n ). Cependant, cette reprÃ©sentation est orthogonale et ne reflÃ¨te pas bien la structure du langage :\n- Les tokens correspondant Ã  des synonymes sont tout aussi distants que des antonymes, alors quâ€™on souhaiterait au contraire modÃ©liser leur proximitÃ© sÃ©mantique.\n- Une meilleure approche consisterait Ã  intÃ©grer ces relations sÃ©mantiques en utilisant des reprÃ©sentations vectorielles plus avancÃ©es, comme les word embeddings (ex. Word2Vec, GloVe, BERT).\n\n\n\n\nPlusieurs librairies, en gÃ©nÃ©ral en python, permettent de tokenizer un texte :\n\nNLTK\nSpacy\nHuggingFace",
    "crumbs": [
      "Qu'est ce que la tokenization ?"
    ]
  },
  {
    "objectID": "2_tokenization.html#quest-ce-que-la-tokenization",
    "href": "2_tokenization.html#quest-ce-que-la-tokenization",
    "title": "Tokenization : premiÃ¨re Ã©tape du NLP",
    "section": "",
    "text": "La tokenisation est le processus de dÃ©coupage dâ€™un long Ã©lÃ©ment en plusieurs petits Ã©lÃ©ments. Elle est essentielle en NLP (traitement du langage naturel) afin de fournir progressivement des fragments dâ€™information Ã  la machine. Quel que soit sa longueur, un texte doit Ãªtre segmentÃ© en petits morceaux pour Ãªtre traitÃ© sÃ©quentiellement par lâ€™algorithme.\nLa tokenisation constitue la base du NLP et influence fortement les rÃ©sultats en fonction des choix effectuÃ©s. Traditionnellement, elle sâ€™effectue au niveau des mots ou des sous-mots : une phrase entiÃ¨re est trop volumineuse pour Ãªtre comprise par la machine, tandis que les caractÃ¨res pris individuellement sont trop atomiques pour les langues latines et cyrilliques.\nExemples de tokenisation :\n\nDÃ©coupage en tokens mots :\nâ€œLa tokenisation en NLP est primordiale.â€ â†’ \"La\", \"tokenisation\", \"en\", \"NLP\", \"est\", \"primordiale\"\n\nDÃ©coupage en tokens sous-mots :\nâ€œLa tokenisation en NLP est primordiale.â€ â†’ \"La\", \"token\", \"isation\", \"en\", \"NLP\", \"est\", \"prim\", \"ordiale\"\n\n\n\n\n\n\nUne fois la tokenisation rÃ©alisÃ©e, on peut analyser les occurrences des tokens et observer des rÃ©gularitÃ©s statistiques, telles que :\n- Les mots les plus frÃ©quents (top mots).\n- Les mots qui apparaissent souvent ensemble, formant des expressions courantes.\n- Les mots qui reviennent dans des contextes similaires.\nEn NLP, on formule lâ€™hypothÃ¨se distributionnelle suivante : les mots qui apparaissent souvent dans des contextes similaires sont proches et a priori ont de fortes chances dâ€™avoir le mÃªme sens.\n\n\n\nLa machine ne comprend que des suites de 0 et 1. Il est donc nÃ©cessaire de convertir chaque token en une reprÃ©sentation numÃ©rique. Pour ce faire, on lui attribue un vecteur numÃ©rique de taille n (suffisamment grand, mais pas excessif).\nUne premiÃ¨re approche, appelÃ©e one-hot encoding, consiste Ã  associer Ã  chaque token un vecteur canonique ( e_n ). Cependant, cette reprÃ©sentation est orthogonale et ne reflÃ¨te pas bien la structure du langage :\n- Les tokens correspondant Ã  des synonymes sont tout aussi distants que des antonymes, alors quâ€™on souhaiterait au contraire modÃ©liser leur proximitÃ© sÃ©mantique.\n- Une meilleure approche consisterait Ã  intÃ©grer ces relations sÃ©mantiques en utilisant des reprÃ©sentations vectorielles plus avancÃ©es, comme les word embeddings (ex. Word2Vec, GloVe, BERT).\n\n\n\n\nPlusieurs librairies, en gÃ©nÃ©ral en python, permettent de tokenizer un texte :\n\nNLTK\nSpacy\nHuggingFace",
    "crumbs": [
      "Qu'est ce que la tokenization ?"
    ]
  },
  {
    "objectID": "3_preprocessing.html",
    "href": "3_preprocessing.html",
    "title": "Les Ã©tapes du pre-processing",
    "section": "",
    "text": "Le prÃ©processing consiste Ã  nettoyer et prÃ©parer un texte avant son traitement par des algorithmes de machine learning. Il permet dâ€™amÃ©liorer la qualitÃ© des donnÃ©es et dâ€™optimiser les performances des modÃ¨les de NLP (traitement du langage naturel).\n\n\n\nLa tokenisation fait partie du prÃ©processing. Elle permet de segmenter un texte en unitÃ©s plus petites (mots, sous-mots ou caractÃ¨res) afin de faciliter lâ€™analyse. Cette Ã©tape est essentielle pour rÃ©aliser des analyses statistiques exploratoires, comme identifier le thÃ¨me dâ€™un texte ou analyser les sentiments.\n\n\n\nLa normalisation consiste Ã  uniformiser lâ€™Ã©criture dâ€™un texte afin de regrouper des tokens similaires. Cela inclut :\n- La conversion en majuscules ou minuscules pour Ã©viter de traiter â€œParisâ€ et â€œparisâ€ comme deux mots distincts.\n- La suppression des accents (ex. Ã© â†’ e) pour uniformiser les termes.\n\n\n\nLes stopwords sont des mots qui nâ€™apportent pas dâ€™information significative sur le sens du texte, mais servent uniquement Ã  la structure syntaxique. Il sâ€™agit notamment des articles (le, la, un, des), des prÃ©positions (Ã , de, pour), ou encore des conjonctions (et, mais, donc). Leur suppression permet de rÃ©duire le bruit et dâ€™amÃ©liorer lâ€™efficacitÃ© du modÃ¨le.\n\n\n\n\nLa lemmatisation consiste Ã  rÃ©duire un mot Ã  sa forme canonique (ou lemme), câ€™est-Ã -dire sa version trouvÃ©e dans le dictionnaire.\n\nEx. â€œmangÃ©â€, â€œmangeonsâ€, â€œmangeaientâ€ â†’ â€œmangerâ€\n\n\nLa stemmatisation, plus rudimentaire, tronque les mots Ã  leur racine commune sans tenir compte des rÃ¨gles linguistiques.\n\nEx. â€œmangerâ€, â€œmangÃ©â€, â€œmangeonsâ€ â†’ â€œmangâ€\n\n\nCes techniques permettent de regrouper les variantes dâ€™un mÃªme mot et dâ€™amÃ©liorer lâ€™analyse du texte.\n\n\n\nToutes ces Ã©tapes sont adaptÃ©es en fonction de la problÃ©matique. Il nâ€™existe pas de mÃ©thode unique, mais plutÃ´t des approches variÃ©es selon les besoins du projet. Il est donc essentiel de tester plusieurs stratÃ©gies afin dâ€™identifier la plus efficace.",
    "crumbs": [
      "PrÃ©traitement des donnÃ©es"
    ]
  },
  {
    "objectID": "3_preprocessing.html#les-Ã©tapes-du-pre-processing",
    "href": "3_preprocessing.html#les-Ã©tapes-du-pre-processing",
    "title": "Les Ã©tapes du pre-processing",
    "section": "",
    "text": "Le prÃ©processing consiste Ã  nettoyer et prÃ©parer un texte avant son traitement par des algorithmes de machine learning. Il permet dâ€™amÃ©liorer la qualitÃ© des donnÃ©es et dâ€™optimiser les performances des modÃ¨les de NLP (traitement du langage naturel).\n\n\n\nLa tokenisation fait partie du prÃ©processing. Elle permet de segmenter un texte en unitÃ©s plus petites (mots, sous-mots ou caractÃ¨res) afin de faciliter lâ€™analyse. Cette Ã©tape est essentielle pour rÃ©aliser des analyses statistiques exploratoires, comme identifier le thÃ¨me dâ€™un texte ou analyser les sentiments.\n\n\n\nLa normalisation consiste Ã  uniformiser lâ€™Ã©criture dâ€™un texte afin de regrouper des tokens similaires. Cela inclut :\n- La conversion en majuscules ou minuscules pour Ã©viter de traiter â€œParisâ€ et â€œparisâ€ comme deux mots distincts.\n- La suppression des accents (ex. Ã© â†’ e) pour uniformiser les termes.\n\n\n\nLes stopwords sont des mots qui nâ€™apportent pas dâ€™information significative sur le sens du texte, mais servent uniquement Ã  la structure syntaxique. Il sâ€™agit notamment des articles (le, la, un, des), des prÃ©positions (Ã , de, pour), ou encore des conjonctions (et, mais, donc). Leur suppression permet de rÃ©duire le bruit et dâ€™amÃ©liorer lâ€™efficacitÃ© du modÃ¨le.\n\n\n\n\nLa lemmatisation consiste Ã  rÃ©duire un mot Ã  sa forme canonique (ou lemme), câ€™est-Ã -dire sa version trouvÃ©e dans le dictionnaire.\n\nEx. â€œmangÃ©â€, â€œmangeonsâ€, â€œmangeaientâ€ â†’ â€œmangerâ€\n\n\nLa stemmatisation, plus rudimentaire, tronque les mots Ã  leur racine commune sans tenir compte des rÃ¨gles linguistiques.\n\nEx. â€œmangerâ€, â€œmangÃ©â€, â€œmangeonsâ€ â†’ â€œmangâ€\n\n\nCes techniques permettent de regrouper les variantes dâ€™un mÃªme mot et dâ€™amÃ©liorer lâ€™analyse du texte.\n\n\n\nToutes ces Ã©tapes sont adaptÃ©es en fonction de la problÃ©matique. Il nâ€™existe pas de mÃ©thode unique, mais plutÃ´t des approches variÃ©es selon les besoins du projet. Il est donc essentiel de tester plusieurs stratÃ©gies afin dâ€™identifier la plus efficace.",
    "crumbs": [
      "PrÃ©traitement des donnÃ©es"
    ]
  },
  {
    "objectID": "3_preprocessing.html#travaux-pratiques",
    "href": "3_preprocessing.html#travaux-pratiques",
    "title": "Les Ã©tapes du pre-processing",
    "section": "Travaux pratiques",
    "text": "Travaux pratiques",
    "crumbs": [
      "PrÃ©traitement des donnÃ©es"
    ]
  }
]
---
title: "LLM et RAG"
author: "raniafhm"
date: "2025-05-09"
format: html
---

## LLM

Les Large Language Models (LLM) sont des réseaux de neurones de très grande taille, comptant souvent plusieurs milliards de paramètres, également appelés poids (correspondant à ceux des neurones). Ces modèles sont entraînés sur d'immenses volumes de données, incluant une grande partie du contenu disponible sur Internet. Ils s’appuient principalement sur l’architecture des Transformers, une structure spécifique de réseau de neurones composée de plusieurs modules empilés.

L'objectif des Transformers est d’apprendre à prédire les tokens (unités de texte) de manière conditionnelle, c’est-à-dire en fonction des tokens précédents. Cette architecture surpasse les modèles bidirectionnels traditionnels, notamment grâce à l’introduction des têtes d’attention. Ces mécanismes permettent au modèle de concentrer plus ou moins d’attention sur certains tokens en fonction du contexte. Par exemple, une tête d’attention peut se spécialiser dans la structure grammaticale d’une phrase, en mettant l’accent sur la relation entre le sujet, le verbe et le complément (les tokens correspondant auront alors un poids plus important que les stopwords par exemple).

e.g : GPT, Gemini, Claude, Llama, Mistral

## RAG

Les LLM (Large Language Models) disposent d'une capacité limitée — bien que conséquente — à traiter des données en entrée, appelée taille d’input (input_size). Cette contrainte signifie que certains documents, trop volumineux, ne peuvent pas être analysés en une seule fois. Par ailleurs, même lorsqu’un document entre entièrement dans cette capacité, il est souvent inutile d’en exploiter l’ensemble pour accomplir une tâche spécifique, comme répondre à une question. Limiter la quantité de texte traité permet également de réduire le coût computationnel, puisque plus le nombre de tokens est élevé, plus les calculs sont complexes et onéreux.

L’architecture Retrieval-Augmented Generation (RAG) apporte une solution efficace à ce problème. Elle consiste à découper un document en segments plus petits, appelés chunks, et à ne transmettre au modèle que les parties les plus pertinentes pour la tâche demandée.

Le processus se déroule généralement en quatre étapes :

0. Découpage et vectorisation (Embedding) : Le document est segmenté selon différentes stratégies (par paragraphe, tous les X caractères, ou via un séparateur spécifique). Chaque segment est ensuite transformé en vecteur dans un espace vectoriel à l’aide d’un embedder.

1. Recherche des segments pertinents (Retrieval) : Lorsqu’une question est posée, elle est également convertie en vecteur, puis comparée aux vecteurs des segments. On sélectionne généralement les 5 à 10 chunks les plus proches.

2. Construction de la question augmentée (Augmentation) : La question est enrichie avec les chunks les plus pertinents afin de fournir au modèle le contexte utile.

3. Génération de la réponse (Generation) : Le LLM produit une réponse en s’appuyant sur la question augmentée et les éléments de contexte fournis.


## le RAG en pratique

La mise en place d'une architecture Retrieval-Augmented Generation (RAG) nécessite de choisir les bons outils et de définir un pipeline clair.

Tout d'abord, pour implémenter le chaînage des différentes étapes, deux frameworks populaires s'offrent à vous : Langchain et LlamaIndex. Langchain est très flexible et permet de créer des chaînes complexes, parfait pour les projets nécessitant une personnalisation fine. En revanche, LlamaIndex est plus simple, conçu pour gérer des données structurées et non structurées, idéal pour des tâches de récupération de documents.

Ensuite, il faut choisir une base de données vectorielle pour stocker et récupérer les vecteurs d'embarquement. Parmi les options disponibles, FAISS est rapide et léger, parfait pour des déploiements locaux ou des projets open-source. Qdrant est plus robuste, adapté à une utilisation à grande échelle, avec une API facile à intégrer. Enfin, ChromaDB est intégré à Langchain et convient bien pour des prototypes ou des projets de taille moyenne.

Une fois les outils choisis, il convient de définir l’architecture. Le processus commence par le découpage du document en chunks, qui sont ensuite convertis en vecteurs via un embedder. Ces vecteurs sont stockés dans la base vectorielle. Lorsque l'utilisateur pose une question, elle est transformée en vecteur et comparée aux vecteurs stockés pour retrouver les chunks les plus pertinents. Ces derniers sont combinés avec la question pour créer une question augmentée. Enfin, cette question enrichie est envoyée à un LLM (comme GPT-4 ou Claude) pour générer une réponse.

Cette architecture optimise l'efficacité du modèle en ne fournissant que les informations pertinentes, ce qui réduit le coût computationnel et améliore la qualité des réponses. Elle est également flexible, permettant de personnaliser chaque étape du processus en fonction des besoins spécifiques du projet.

## Implémentation minimal avec Langchain et ChromaDB

* Vectorisation
```
FILE="10p_accords_publics_et_thematiques_240815.parquet"

import pandas as pd
from langchain_huggingface import HuggingFaceEmbeddings
from langchain.vectorstores import Chroma
from langchain_chroma import Chroma
from langchain_text_splitters import CharacterTextSplitter
from tqdm import tqdm

text_splitter = CharacterTextSplitter(
    separator="\n\n",
    chunk_size=3000,
    chunk_overlap=200,
    length_function=len,
    is_separator_regex=False,
)



model_kwargs = {'device': 'cuda'}
embedder = HuggingFaceEmbeddings(model_name="BAAI/bge-m3", model_kwargs=model_kwargs,show_progress=False)

df=pd.read_parquet(FILE)

vector_store = Chroma(embedding_function=embedder, persist_directory="./chroma_db")
for index, row in tqdm(df.iterrows(), total=len(df)):
    text=df.loc[index].texte
    texts = text_splitter.create_documents([text])
    i=0
    for t in texts:
        t.metadata["id"]=f"{index}_{i}"
        t.metadata["index"]=f"{index}"
        vector_store.add_documents([t])
        i+=1
```

* RAG
```
import json
import numpy as np
import pandas as pd
import requests

from langchain.chains import StuffDocumentsChain, RetrievalQA, LLMChain, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.document_loaders import TextLoader
from langchain.embeddings import OpenAIEmbeddings, OllamaEmbeddings
from langchain.llms import Ollama, BaseLLM
from langchain.schema import Document, Generation, LLMResult
from langchain.vectorstores import Chroma
from langchain_chroma import Chroma
from langchain_community.llms import OpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_huggingface import HuggingFaceEmbeddings
from langchain_text_splitters import CharacterTextSplitter
from pathlib import Path
from tqdm import tqdm
from glob import glob

class LocalOllamaLLM(BaseLLM):
    api_url : str
    def _generate(self, prompt, stop):
        response = requests.post(f"{self.api_url}/api/generate", json={"model": "llama3.1", "prompt": str(prompt) })
        response.raise_for_status()
        response_text=''.join([json.loads(line)['response'] for line in response.text.splitlines()])
        generations=[]
        generations.append([Generation(text=response_text)])
        return LLMResult(generations=generations)


    def _llm_type(self):
        return "local"  # Or whatever type is appropriate for your local setup

llm = LocalOllamaLLM(api_url="http://127.0.0.1:11434")

embedder = HuggingFaceEmbeddings(model_name="BAAI/bge-m3")
vector_store = Chroma(embedding_function=embedder, persist_directory="./chroma_db")


system_prompt = (
    " Répondez à la question posée "
    " Utilisez le contexte (sélection des meilleurs paragraphes liés à la question) donné pour répondre à la question "
    " Si la réponse ne se trouve pas dans le contexte, répondez par 'Je ne sais pas'"
    " Contexte : {context}  "
)

prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        ("human", "{input}"),
    ]
)
question_answer_chain = create_stuff_documents_chain(llm, prompt)


def search_and_invoke_llm(vector_store,index,query,k=5):
    if k==0:
        print(f"bug with {index}")
        return None
    else:
        pass
    try:
        retriever=vector_store.as_retriever(
        search_kwargs={
                "k": k, 
                "filter": {'index': index}
            }
        )
        chain = create_retrieval_chain(retriever, question_answer_chain)
        result=chain.invoke({"input": query})
        return result
    except:
        search_and_invoke_llm(vector_store,index,query,k=k-1)
    return None
	
QUESTIONS=["Ma question : "]	
	
list_of_df=[]
Path("results").mkdir(parents=True, exist_ok=True)
for index, row in df.iterrows():
    dict_answer=dict()
    answer=""
    for Q0 in QUESTIONS:
        if ans:=search_and_invoke_llm(vector_store,index,Q0,k=2):
            answer_txt=ans['answer']
            answer+=answer_txt

    with open(f"results/{index}.answer","w") as file:
        file.write(answer)
```



## Travaux pratiques
---
title: "Du mot au vecteur"
author: "raniafhm"
date: "2025-17-03"
format: html
---

## Embedding : Passer du token au vecteur numÃ©rique

### DÃ©finition  

Un **embedding** est un espace vectoriel permettant dâ€™attribuer une reprÃ©sentation numÃ©rique Ã  un token. GrÃ¢ce Ã  ces reprÃ©sentations, la machine peut effectuer des opÃ©rations arithmÃ©tiques pour manipuler le langage et dÃ©terminer si des mots sont proches ou non sur le plan sÃ©mantique.  

### L'embedding naÃ¯f  

Avec un vocabulaire de tokens de taille \( n \), on peut crÃ©er un **espace vectoriel de dimension \( n \)**, oÃ¹ chaque token est reprÃ©sentÃ© par un **vecteur unitaire canonique** (one-hot encoding). Cependant, cette approche prÃ©sente plusieurs inconvÃ©nients :  
1. Lâ€™espace vectoriel devient **trÃ¨s grand** lorsque \( n \) est important.  
2. Les reprÃ©sentations sont **orthogonales**, ce qui signifie qu'elles ne capturent **aucune relation sÃ©mantique** entre les mots.  

### Apprentissage des relations sÃ©mantiques  

En sâ€™appuyant sur **lâ€™hypothÃ¨se distributionnelle** et les **rÃ©seaux de neurones**, les modÃ¨les neuronaux **Bag of Words** (CBOW et Skip-Gram) permettent dâ€™apprendre des **reprÃ©sentations vectorielles compressÃ©es** tout en conservant les **proximitÃ©s et distances sÃ©mantiques** entre les tokens.  

## Les premiers algorithmes sÃ©mantiques  

### ğŸ”¹ L'algorithme CBOW  

L'algorithme **Continuous Bag of Words (CBOW)** repose sur un **rÃ©seau de neurones** composÃ© de deux Ã©tapes :  
1. **Encodage** : une couche dâ€™entrÃ©e qui compresse les mots du contexte dans un vecteur de taille rÃ©duite.  
2. **DÃ©codage** : une couche de sortie qui prend ce vecteur rÃ©duit et prÃ©dit le mot cible.  

Lâ€™idÃ©e est dâ€™entraÃ®ner le rÃ©seau Ã  **prÃ©dire un mot** Ã  partir de son **contexte** (les mots qui lâ€™entourent).  

**Exemple avec un contexte de 4 mots** (*les deux mots avant et aprÃ¨s*) :  
- Phrase : *"Le chat dort sur le canapÃ©."*  
- Contexte : `["Le", "dort", "sur", "le"]`  
- Mot Ã  prÃ©dire : `"chat"`  
- Base d'entraÃ®nement : (("Le", "dort", "sur", "le"), "chat")

En parcourant un corpus, lâ€™algorithme apprend ces associations de maniÃ¨re probabiliste. Une fois lâ€™entraÃ®nement terminÃ©, on **rÃ©cupÃ¨re la reprÃ©sentation vectorielle intermÃ©diaire** pour associer chaque mot Ã  son embedding.  

### ğŸ”¹ L'algorithme Skip-Gram  

L'architecture du rÃ©seau est similaire Ã  CBOW, mais avec une approche inversÃ©e :  
- PlutÃ´t que de **prÃ©dire un mot Ã  partir de son contexte**, Skip-Gram **prÃ©dit les mots du contexte Ã  partir dâ€™un mot donnÃ©**.  
- On gÃ©nÃ¨re des paires (mot, mot_contexte) en fonction de la fenÃªtre de contexte choisie.  

**Exemple :**  
- Phrase : *"Le chat dort sur le canapÃ©."*  
- Mot donnÃ© : `"chat"`  
- Contexte Ã  prÃ©dire : `["Le", "dort", "sur", "le"]`  
- Base d'entraÃ®nement : ("Le","chat") ; ("dort","chat") ; ("sur","chat") ; ("le","chat")

Une fois la distribution apprise, on dÃ©branche la deuxiÃ¨me couche et on utilise la rÃ©prÃ©sentation vectorielle de la couche cachÃ©e.

### ğŸ”¹ L'algorithme GloVe  

L'algorithme **GloVe (Global Vectors for Word Representation)** adopte une approche diffÃ©rente :  
- PlutÃ´t que dâ€™analyser des contextes locaux (comme CBOW et Skip-Gram), GloVe **apprend des cooccurrences de mots** Ã  partir dâ€™un large corpus.  
- Il construit une **matrice de cooccurrence** indiquant Ã  quelle frÃ©quence deux mots apparaissent ensemble dans les mÃªmes phrases.  
- Ensuite, un **facteur de dÃ©composition** est utilisÃ© pour gÃ©nÃ©rer des reprÃ©sentations vectorielles capturant les relations sÃ©mantiques.  

GloVe est particuliÃ¨rement efficace pour reprÃ©senter **des mots ayant des relations sÃ©mantiques globales**, comme :  
- *Roi â€“ Reine*,  
- *France â€“ Paris*,  
- *Banque â€“ Argent*.  

### ğŸ”¹ L'algorithme FastText  

FastText est une amÃ©lioration de CBOW et Skip-Gram :  
- **Au lieu de reprÃ©senter un mot en entier, il le dÃ©compose en sous-mots (n-grams).**  
- Cela permet de mieux gÃ©rer les **mots rares ou inconnus** en gÃ©nÃ©rant des embeddings dynamiques.  

**Exemple :**  
- Le mot *"apprentissage"* peut Ãªtre dÃ©composÃ© en `["app", "pren", "tiss", "age"]`.  
- Si un mot jamais vu auparavant est rencontrÃ©, son embedding peut Ãªtre infÃ©rÃ© Ã  partir de ses sous-mots.  

FastText est particuliÃ¨rement utile pour **les langues morphologiquement riches** (comme lâ€™allemand ou le turc) et pour gÃ©rer les **fautes dâ€™orthographe ou les variantes linguistiques**.  


## Les embeddings contextuels

Le principal problÃ¨me des embeddings classiques est que leur **reprÃ©sentation est fixe** : un mÃªme token possÃ¨de toujours le **mÃªme vecteur numÃ©rique**, quel que soit le contexte. Or, dans de nombreuses langues, notamment en franÃ§ais, **un mÃªme mot peut avoir plusieurs significations** selon son usage. De plus, **des notions diffÃ©rentes peuvent sâ€™Ã©crire de la mÃªme faÃ§on** (homonymes).  

Pour dÃ©sambiguÃ¯ser ces cas, il est essentiel de prendre en compte **le contexte** dans lequel un mot apparaÃ®t afin de dÃ©terminer son sens prÃ©cis.  

Les **embeddings contextuels** permettent dâ€™adapter la reprÃ©sentation numÃ©rique dâ€™un mot **en fonction de son contexte**. Contrairement aux approches statiques, ils tiennent compte **de la position et de l'interaction entre les mots** dans une phrase.  

**Exemple :**  
- *"La **baleine** nage dans lâ€™ocÃ©an."* ğŸ‹ â†’ **baleine (animal)**  
- *"Jâ€™ai un mal de **baleine** aprÃ¨s ce repas."* ğŸ¤• â†’ **baleine (expression signifiant une douleur intense)**  

Les modÃ¨les basÃ©s sur les **transformers** (ex. **BERT, GPT, T5**) sont capables dâ€™encoder ces diffÃ©rences en gÃ©nÃ©rant un **vecteur unique pour chaque occurrence dâ€™un mot** selon son contexte. Ainsi, un mot aura un embedding spÃ©cifique **qui varie en fonction des mots qui l'entourent**, tout en **conservant les relations sÃ©mantiques** apprises (synonymie, antonymie, etc.).

## Travaux pratiques
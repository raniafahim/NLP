[
  {
    "objectID": "3_preprocessing.html",
    "href": "3_preprocessing.html",
    "title": "Les √©tapes du pre-processing",
    "section": "",
    "text": "Le pr√©processing consiste √† nettoyer et pr√©parer un texte avant son traitement par des algorithmes de machine learning. Il permet d‚Äôam√©liorer la qualit√© des donn√©es et d‚Äôoptimiser les performances des mod√®les de NLP (traitement du langage naturel).\n\n\n\nLa tokenisation fait partie du pr√©processing. Elle permet de segmenter un texte en unit√©s plus petites (mots, sous-mots ou caract√®res) afin de faciliter l‚Äôanalyse. Cette √©tape est essentielle pour r√©aliser des analyses statistiques exploratoires, comme identifier le th√®me d‚Äôun texte ou analyser les sentiments.\n\n\n\nLa normalisation consiste √† uniformiser l‚Äô√©criture d‚Äôun texte afin de regrouper des tokens similaires. Cela inclut :\n- La conversion en majuscules ou minuscules pour √©viter de traiter ‚ÄúParis‚Äù et ‚Äúparis‚Äù comme deux mots distincts.\n- La suppression des accents (ex. √© ‚Üí e) pour uniformiser les termes.\n\n\n\nLes stopwords sont des mots qui n‚Äôapportent pas d‚Äôinformation significative sur le sens du texte, mais servent uniquement √† la structure syntaxique. Il s‚Äôagit notamment des articles (le, la, un, des), des pr√©positions (√†, de, pour), ou encore des conjonctions (et, mais, donc). Leur suppression permet de r√©duire le bruit et d‚Äôam√©liorer l‚Äôefficacit√© du mod√®le.\n\n\n\n\nLa lemmatisation consiste √† r√©duire un mot √† sa forme canonique (ou lemme), c‚Äôest-√†-dire sa version trouv√©e dans le dictionnaire.\n\nEx. ‚Äúmang√©‚Äù, ‚Äúmangeons‚Äù, ‚Äúmangeaient‚Äù ‚Üí ‚Äúmanger‚Äù\n\n\nLa stemmatisation, plus rudimentaire, tronque les mots √† leur racine commune sans tenir compte des r√®gles linguistiques.\n\nEx. ‚Äúmanger‚Äù, ‚Äúmang√©‚Äù, ‚Äúmangeons‚Äù ‚Üí ‚Äúmang‚Äù\n\n\nCes techniques permettent de regrouper les variantes d‚Äôun m√™me mot et d‚Äôam√©liorer l‚Äôanalyse du texte.\n\n\n\nToutes ces √©tapes sont adapt√©es en fonction de la probl√©matique. Il n‚Äôexiste pas de m√©thode unique, mais plut√¥t des approches vari√©es selon les besoins du projet. Il est donc essentiel de tester plusieurs strat√©gies afin d‚Äôidentifier la plus efficace.",
    "crumbs": [
      "Pr√©traitement des donn√©es"
    ]
  },
  {
    "objectID": "3_preprocessing.html#les-√©tapes-du-pre-processing",
    "href": "3_preprocessing.html#les-√©tapes-du-pre-processing",
    "title": "Les √©tapes du pre-processing",
    "section": "",
    "text": "Le pr√©processing consiste √† nettoyer et pr√©parer un texte avant son traitement par des algorithmes de machine learning. Il permet d‚Äôam√©liorer la qualit√© des donn√©es et d‚Äôoptimiser les performances des mod√®les de NLP (traitement du langage naturel).\n\n\n\nLa tokenisation fait partie du pr√©processing. Elle permet de segmenter un texte en unit√©s plus petites (mots, sous-mots ou caract√®res) afin de faciliter l‚Äôanalyse. Cette √©tape est essentielle pour r√©aliser des analyses statistiques exploratoires, comme identifier le th√®me d‚Äôun texte ou analyser les sentiments.\n\n\n\nLa normalisation consiste √† uniformiser l‚Äô√©criture d‚Äôun texte afin de regrouper des tokens similaires. Cela inclut :\n- La conversion en majuscules ou minuscules pour √©viter de traiter ‚ÄúParis‚Äù et ‚Äúparis‚Äù comme deux mots distincts.\n- La suppression des accents (ex. √© ‚Üí e) pour uniformiser les termes.\n\n\n\nLes stopwords sont des mots qui n‚Äôapportent pas d‚Äôinformation significative sur le sens du texte, mais servent uniquement √† la structure syntaxique. Il s‚Äôagit notamment des articles (le, la, un, des), des pr√©positions (√†, de, pour), ou encore des conjonctions (et, mais, donc). Leur suppression permet de r√©duire le bruit et d‚Äôam√©liorer l‚Äôefficacit√© du mod√®le.\n\n\n\n\nLa lemmatisation consiste √† r√©duire un mot √† sa forme canonique (ou lemme), c‚Äôest-√†-dire sa version trouv√©e dans le dictionnaire.\n\nEx. ‚Äúmang√©‚Äù, ‚Äúmangeons‚Äù, ‚Äúmangeaient‚Äù ‚Üí ‚Äúmanger‚Äù\n\n\nLa stemmatisation, plus rudimentaire, tronque les mots √† leur racine commune sans tenir compte des r√®gles linguistiques.\n\nEx. ‚Äúmanger‚Äù, ‚Äúmang√©‚Äù, ‚Äúmangeons‚Äù ‚Üí ‚Äúmang‚Äù\n\n\nCes techniques permettent de regrouper les variantes d‚Äôun m√™me mot et d‚Äôam√©liorer l‚Äôanalyse du texte.\n\n\n\nToutes ces √©tapes sont adapt√©es en fonction de la probl√©matique. Il n‚Äôexiste pas de m√©thode unique, mais plut√¥t des approches vari√©es selon les besoins du projet. Il est donc essentiel de tester plusieurs strat√©gies afin d‚Äôidentifier la plus efficace.",
    "crumbs": [
      "Pr√©traitement des donn√©es"
    ]
  },
  {
    "objectID": "3_preprocessing.html#travaux-pratiques",
    "href": "3_preprocessing.html#travaux-pratiques",
    "title": "Les √©tapes du pre-processing",
    "section": "2 Travaux pratiques",
    "text": "2 Travaux pratiques",
    "crumbs": [
      "Pr√©traitement des donn√©es"
    ]
  },
  {
    "objectID": "6_topic_modeling_BERTopic.html",
    "href": "6_topic_modeling_BERTopic.html",
    "title": "BERTopic",
    "section": "",
    "text": "BERTopic est une biblioth√®que de Topic Modeling qui permet d‚Äôutiliser des connaissances externes pour construire les topics. Elle repose sur une m√©thodologie en deux √©tapes. Dans un premier temps, BERTopic d√©termine le nombre de topics en projetant les documents dans un espace vectoriel (embeddings), en r√©duisant la dimension de cet espace, puis en appliquant un algorithme de clustering. Une fois cette √©tape termin√©e, les clusters (topics) sont form√©s et les documents leur sont attribu√©s. Dans un second temps, BERTopic caract√©rise les topics en analysant les documents associ√©s, afin d‚Äôidentifier les mots les plus fr√©quents sp√©cifiquement pr√©sents dans chaque topic.",
    "crumbs": [
      "Topic Modeling - BERT"
    ]
  },
  {
    "objectID": "6_topic_modeling_BERTopic.html#bertopic",
    "href": "6_topic_modeling_BERTopic.html#bertopic",
    "title": "BERTopic",
    "section": "",
    "text": "BERTopic est une biblioth√®que de Topic Modeling qui permet d‚Äôutiliser des connaissances externes pour construire les topics. Elle repose sur une m√©thodologie en deux √©tapes. Dans un premier temps, BERTopic d√©termine le nombre de topics en projetant les documents dans un espace vectoriel (embeddings), en r√©duisant la dimension de cet espace, puis en appliquant un algorithme de clustering. Une fois cette √©tape termin√©e, les clusters (topics) sont form√©s et les documents leur sont attribu√©s. Dans un second temps, BERTopic caract√©rise les topics en analysant les documents associ√©s, afin d‚Äôidentifier les mots les plus fr√©quents sp√©cifiquement pr√©sents dans chaque topic.",
    "crumbs": [
      "Topic Modeling - BERT"
    ]
  },
  {
    "objectID": "6_topic_modeling_BERTopic.html#m√©thodologie-bertopic",
    "href": "6_topic_modeling_BERTopic.html#m√©thodologie-bertopic",
    "title": "BERTopic",
    "section": "2 M√©thodologie BERTopic",
    "text": "2 M√©thodologie BERTopic\n\n2.1 1 - Plongements lexicaux : du document aux vecteurs\nL‚Äôobjectif est de projeter un document texte dans un espace vectoriel. Cette projection peut √™tre r√©alis√©e soit sur l‚Äôensemble du document, soit en le d√©coupant pr√©alablement en plusieurs sous-documents, qui sont ensuite projet√©s individuellement. Dans ce cas, l‚Äôanalyse est men√©e au niveau des sous-documents. Cette approche est g√©n√©ralement privil√©gi√©e lorsque les documents sont particuli√®rement longs.\n\n\n2.2 2 - R√©duction de dimension : d‚Äôun vecteur long √† un vecteur court\nLe but est de r√©duire la dimension des vecteurs issus du plongement lexical, qui produisent g√©n√©ralement des repr√©sentations de grande taille, souvent sup√©rieures √† 500 dimensions. Pour que le clustering soit efficace, il est pr√©f√©rable de travailler dans des espaces de dimensions r√©duites, typiquement entre 30 et 100. Plusieurs techniques peuvent √™tre employ√©es √† cet effet, comme l‚ÄôAnalyse en Composantes Principales (ACP) ou l‚ÄôUMAP. Ces m√©thodes permettent de conserver les dimensions les plus pertinentes tout en √©liminant le bruit.\n\n\n2.3 3 - Clustering : cr√©ation des topics\nApr√®s la r√©duction de dimension, o√π seules les dimensions les plus significatives sont conserv√©es, BERTopic proc√®de √† un regroupement des documents en appliquant un algorithme de clustering. Les documents similaires sont ainsi organis√©s en clusters, chacun correspondant √† un topic, c‚Äôest-√†-dire une th√©matique coh√©rente d√©duite des proximit√©s observ√©es dans l‚Äôespace vectoriel r√©duit. L‚Äôalgorithme de clustering utilis√©, le plus souvent HDBSCAN, pr√©sente l‚Äôavantage de s‚Äôadapter √† des structures de donn√©es complexes, en identifiant automatiquement le nombre appropri√© de clusters sans param√©trage pr√©alable. √Ä l‚Äôissue de ce processus, chaque document est rattach√© √† un topic en fonction de sa proximit√© avec les autres √©l√©ments du cluster, ouvrant la voie √† l‚Äô√©tape suivante qui consiste √† caract√©riser le contenu s√©mantique de chaque topic.\n\n\n2.4 4 - Analyse des tokens : caract√©risation des topics\nPour chaque topic, un document agr√©g√© est constitu√© √† partir de l‚Äôensemble des documents qui lui sont associ√©s. Une analyse fr√©quentielle des tokens est ensuite r√©alis√©e. BERTopic applique une pond√©ration sp√©cifique aux tokens en utilisant la m√©thode c-TF-IDF (class-based TF-IDF), qui √©value l‚Äôimportance d‚Äôun token en fonction de sa fr√©quence dans le topic par rapport √† l‚Äôensemble du corpus. Cette approche permet de mieux identifier les termes caract√©ristiques de chaque topic, en mettant en avant les tokens qui sont particuli√®rement repr√©sentatifs d‚Äôun groupe donn√©, m√™me s‚Äôils sont peu fr√©quents dans l‚Äôensemble des documents.",
    "crumbs": [
      "Topic Modeling - BERT"
    ]
  },
  {
    "objectID": "6_topic_modeling_BERTopic.html#composition",
    "href": "6_topic_modeling_BERTopic.html#composition",
    "title": "BERTopic",
    "section": "3 Composition",
    "text": "3 Composition\nBERTopic offre une grande flexibilit√©, ce qui lui permet de r√©pondre de mani√®re souple √† n‚Äôimporte quelle probl√©matique. La librairie autorise le choix de chaque algorithme √† chaque √©tape du processus. Pour la g√©n√©ration des plongements lexicaux, il est possible d‚Äôutiliser Sentence-BERT, diff√©rents mod√®les de Transformers ou encore d‚Äôautres m√©thodes d‚Äôencodage adapt√©es aux sp√©cificit√©s du corpus. Pour la r√©duction de dimension, BERTopic utilise par d√©faut l‚Äôalgorithme UMAP, mais il est √©galement possible d‚Äôopter pour d‚Äôautres techniques telles que l‚ÄôAnalyse en Composantes Principales (ACP) ou t-SNE selon les besoins analytiques.\nEn ce qui concerne l‚Äô√©tape de clustering, BERTopic s‚Äôappuie initialement sur HDBSCAN, un algorithme de clustering hi√©rarchique bas√© sur la densit√©, mais l‚Äôutilisateur peut choisir d‚Äôappliquer d‚Äôautres m√©thodes comme K-Means ou DBSCAN en fonction de ses objectifs. De plus, BERTopic propose des fonctionnalit√©s avanc√©es pour guider la d√©couverte des topics √† partir de mots-cl√©s m√©tier, facilitant ainsi une approche semi-supervis√©e, ou pour organiser les topics de mani√®re hi√©rarchique, en cr√©ant des structures imbriqu√©es qui refl√®tent plus finement la complexit√© th√©matique du corpus.\nCette modularit√© rend BERTopic particuli√®rement adapt√© √† des contextes vari√©s, qu‚Äôil s‚Äôagisse d‚Äôexploration de donn√©es, d‚Äôanalyse th√©matique dirig√©e ou de projets n√©cessitant une forte personnalisation des r√©sultats.\n\n\n\nM√©thodologie de BERTopic",
    "crumbs": [
      "Topic Modeling - BERT"
    ]
  },
  {
    "objectID": "6_topic_modeling_BERTopic.html#travaux-pratiques",
    "href": "6_topic_modeling_BERTopic.html#travaux-pratiques",
    "title": "BERTopic",
    "section": "4 Travaux pratiques",
    "text": "4 Travaux pratiques",
    "crumbs": [
      "Topic Modeling - BERT"
    ]
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "Introduction au NLP",
    "section": "",
    "text": "Il existe plusieurs d√©finitions pour d√©finir le concept de langage naturel Lien Dictonnary .\n\nUn ensemble de mots et les syst√®mes de leur utilisation communs √† un peuple appartenant √† la m√™me communaut√© ou nation, √† la m√™me zone g√©ographique ou √† la m√™me tradition culturelle.\nLa communication par la voix de mani√®re sp√©cifiquement humaine, en utilisant des sons arbitraires de fa√ßon conventionnelle avec des significations conventionnelles.\nLe syst√®me de signes ou de symboles linguistiques consid√©r√© dans l‚Äôabstrait (oppos√© √† la parole).\nTout ensemble ou syst√®me de tels symboles utilis√©s de mani√®re plus ou moins uniforme par un certain nombre de personnes, leur permettant ainsi de communiquer intelligiblement entre elles.\nTout syst√®me de symboles formalis√©s, signes, sons, gestes, ou autres, utilis√© ou con√ßu comme un moyen de communiquer la pens√©e, l‚Äô√©motion, etc. : le langage des math√©matiques ; la langue des signes. Les moyens de communication utilis√©s par les animaux.\n\nAinsi, on peut voir le traitement du langage naturel comme un ensemble de recherche et de d√©veloppement visant √† mod√©liser et √† reproduire, √† l‚Äôaide de machines, la capacit√© humaine √† produire et √† comprendre des √©nonc√©s linguistiques √† des fins de communication. Le langage est ainsi un concept complexe et ambigu. Son interpr√©tation s‚Äôaccompagne de nombreuses connaissances implicites (expressions, m√©taphores, m√©tonymies‚Ä¶) et de bon sens, ce qui est tr√®s compliqu√© √† retranscrire √† un ordinateur.\nExemples : J‚Äôai lu un article sur le droit des femmes dans le journal. J‚Äôai lu un article sur le droit des femmes dans le m√©tro.",
    "crumbs": [
      "Introduction au NLP"
    ]
  },
  {
    "objectID": "1_intro.html#quest-ce-que-le-traitement-du-langage-naturel",
    "href": "1_intro.html#quest-ce-que-le-traitement-du-langage-naturel",
    "title": "Introduction au NLP",
    "section": "",
    "text": "Il existe plusieurs d√©finitions pour d√©finir le concept de langage naturel Lien Dictonnary .\n\nUn ensemble de mots et les syst√®mes de leur utilisation communs √† un peuple appartenant √† la m√™me communaut√© ou nation, √† la m√™me zone g√©ographique ou √† la m√™me tradition culturelle.\nLa communication par la voix de mani√®re sp√©cifiquement humaine, en utilisant des sons arbitraires de fa√ßon conventionnelle avec des significations conventionnelles.\nLe syst√®me de signes ou de symboles linguistiques consid√©r√© dans l‚Äôabstrait (oppos√© √† la parole).\nTout ensemble ou syst√®me de tels symboles utilis√©s de mani√®re plus ou moins uniforme par un certain nombre de personnes, leur permettant ainsi de communiquer intelligiblement entre elles.\nTout syst√®me de symboles formalis√©s, signes, sons, gestes, ou autres, utilis√© ou con√ßu comme un moyen de communiquer la pens√©e, l‚Äô√©motion, etc. : le langage des math√©matiques ; la langue des signes. Les moyens de communication utilis√©s par les animaux.\n\nAinsi, on peut voir le traitement du langage naturel comme un ensemble de recherche et de d√©veloppement visant √† mod√©liser et √† reproduire, √† l‚Äôaide de machines, la capacit√© humaine √† produire et √† comprendre des √©nonc√©s linguistiques √† des fins de communication. Le langage est ainsi un concept complexe et ambigu. Son interpr√©tation s‚Äôaccompagne de nombreuses connaissances implicites (expressions, m√©taphores, m√©tonymies‚Ä¶) et de bon sens, ce qui est tr√®s compliqu√© √† retranscrire √† un ordinateur.\nExemples : J‚Äôai lu un article sur le droit des femmes dans le journal. J‚Äôai lu un article sur le droit des femmes dans le m√©tro.",
    "crumbs": [
      "Introduction au NLP"
    ]
  },
  {
    "objectID": "4_embedding.html",
    "href": "4_embedding.html",
    "title": "Du mot au vecteur",
    "section": "",
    "text": "Un embedding est un espace vectoriel permettant d‚Äôattribuer une repr√©sentation num√©rique √† un token. Gr√¢ce √† ces repr√©sentations, la machine peut effectuer des op√©rations arithm√©tiques pour manipuler le langage et d√©terminer si des mots sont proches ou non sur le plan s√©mantique.\n\n\n\nAvec un vocabulaire de tokens de taille ( n ), on peut cr√©er un espace vectoriel de dimension ( n ), o√π chaque token est repr√©sent√© par un vecteur unitaire canonique (one-hot encoding). Cependant, cette approche pr√©sente plusieurs inconv√©nients :\n1. L‚Äôespace vectoriel devient tr√®s grand lorsque ( n ) est important.\n2. Les repr√©sentations sont orthogonales, ce qui signifie qu‚Äôelles ne capturent aucune relation s√©mantique entre les mots.\n\n\n\nEn s‚Äôappuyant sur l‚Äôhypoth√®se distributionnelle et les r√©seaux de neurones, les mod√®les neuronaux Bag of Words (CBOW et Skip-Gram) permettent d‚Äôapprendre des repr√©sentations vectorielles compress√©es tout en conservant les proximit√©s et distances s√©mantiques entre les tokens.",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#embedding-passer-du-token-au-vecteur-num√©rique",
    "href": "4_embedding.html#embedding-passer-du-token-au-vecteur-num√©rique",
    "title": "Du mot au vecteur",
    "section": "",
    "text": "Un embedding est un espace vectoriel permettant d‚Äôattribuer une repr√©sentation num√©rique √† un token. Gr√¢ce √† ces repr√©sentations, la machine peut effectuer des op√©rations arithm√©tiques pour manipuler le langage et d√©terminer si des mots sont proches ou non sur le plan s√©mantique.\n\n\n\nAvec un vocabulaire de tokens de taille ( n ), on peut cr√©er un espace vectoriel de dimension ( n ), o√π chaque token est repr√©sent√© par un vecteur unitaire canonique (one-hot encoding). Cependant, cette approche pr√©sente plusieurs inconv√©nients :\n1. L‚Äôespace vectoriel devient tr√®s grand lorsque ( n ) est important.\n2. Les repr√©sentations sont orthogonales, ce qui signifie qu‚Äôelles ne capturent aucune relation s√©mantique entre les mots.\n\n\n\nEn s‚Äôappuyant sur l‚Äôhypoth√®se distributionnelle et les r√©seaux de neurones, les mod√®les neuronaux Bag of Words (CBOW et Skip-Gram) permettent d‚Äôapprendre des repr√©sentations vectorielles compress√©es tout en conservant les proximit√©s et distances s√©mantiques entre les tokens.",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#les-premiers-algorithmes-s√©mantiques",
    "href": "4_embedding.html#les-premiers-algorithmes-s√©mantiques",
    "title": "Du mot au vecteur",
    "section": "2 Les premiers algorithmes s√©mantiques",
    "text": "2 Les premiers algorithmes s√©mantiques\n\n2.1 L‚Äôalgorithme CBOW\nL‚Äôalgorithme Continuous Bag of Words (CBOW) repose sur un r√©seau de neurones compos√© de deux √©tapes :\n1. Encodage : une couche d‚Äôentr√©e qui compresse les mots du contexte dans un vecteur de taille r√©duite.\n2. D√©codage : une couche de sortie qui prend ce vecteur r√©duit et pr√©dit le mot cible.\nL‚Äôid√©e est d‚Äôentra√Æner le r√©seau √† pr√©dire un mot √† partir de son contexte (les mots qui l‚Äôentourent).\nExemple avec un contexte de 4 mots (les deux mots avant et apr√®s) :\n- Phrase : ‚ÄúLe chat dort sur le canap√©.‚Äù\n- Contexte : [\"Le\", \"dort\", \"sur\", \"le\"]\n- Mot √† pr√©dire : \"chat\"\n- Base d‚Äôentra√Ænement : ((‚ÄúLe‚Äù, ‚Äúdort‚Äù, ‚Äúsur‚Äù, ‚Äúle‚Äù), ‚Äúchat‚Äù)\nEn parcourant un corpus, l‚Äôalgorithme apprend ces associations de mani√®re probabiliste. Une fois l‚Äôentra√Ænement termin√©, on r√©cup√®re la repr√©sentation vectorielle interm√©diaire pour associer chaque mot √† son embedding.\n\n\n2.2 L‚Äôalgorithme Skip-Gram\nL‚Äôarchitecture du r√©seau est similaire √† CBOW, mais avec une approche invers√©e :\n- Plut√¥t que de pr√©dire un mot √† partir de son contexte, Skip-Gram pr√©dit les mots du contexte √† partir d‚Äôun mot donn√©.\n- On g√©n√®re des paires (mot, mot_contexte) en fonction de la fen√™tre de contexte choisie.\nExemple :\n- Phrase : ‚ÄúLe chat dort sur le canap√©.‚Äù\n- Mot donn√© : \"chat\"\n- Contexte √† pr√©dire : [\"Le\", \"dort\", \"sur\", \"le\"]\n- Base d‚Äôentra√Ænement : (‚ÄúLe‚Äù,‚Äúchat‚Äù) ; (‚Äúdort‚Äù,‚Äúchat‚Äù) ; (‚Äúsur‚Äù,‚Äúchat‚Äù) ; (‚Äúle‚Äù,‚Äúchat‚Äù)\nUne fois la distribution apprise, on d√©branche la deuxi√®me couche et on utilise la r√©pr√©sentation vectorielle de la couche cach√©e.\n\n\n2.3 L‚Äôalgorithme GloVe\nL‚Äôalgorithme GloVe (Global Vectors for Word Representation) adopte une approche diff√©rente :\n- Plut√¥t que d‚Äôanalyser des contextes locaux (comme CBOW et Skip-Gram), GloVe apprend des cooccurrences de mots √† partir d‚Äôun large corpus.\n- Il construit une matrice de cooccurrence indiquant √† quelle fr√©quence deux mots apparaissent ensemble dans les m√™mes phrases.\n- Ensuite, un facteur de d√©composition est utilis√© pour g√©n√©rer des repr√©sentations vectorielles capturant les relations s√©mantiques.\nGloVe est particuli√®rement efficace pour repr√©senter des mots ayant des relations s√©mantiques globales, comme :\n- Roi ‚Äì Reine,\n- France ‚Äì Paris,\n- Banque ‚Äì Argent.\n\n\n2.4 L‚Äôalgorithme FastText\nFastText est une am√©lioration de CBOW et Skip-Gram :\n- Au lieu de repr√©senter un mot en entier, il le d√©compose en sous-mots (n-grams).\n- Cela permet de mieux g√©rer les mots rares ou inconnus en g√©n√©rant des embeddings dynamiques.\nExemple :\n- Le mot ‚Äúapprentissage‚Äù peut √™tre d√©compos√© en [\"app\", \"pren\", \"tiss\", \"age\"].\n- Si un mot jamais vu auparavant est rencontr√©, son embedding peut √™tre inf√©r√© √† partir de ses sous-mots.\nFastText est particuli√®rement utile pour les langues morphologiquement riches (comme l‚Äôallemand ou le turc) et pour g√©rer les fautes d‚Äôorthographe ou les variantes linguistiques.",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#les-embeddings-contextuels",
    "href": "4_embedding.html#les-embeddings-contextuels",
    "title": "Du mot au vecteur",
    "section": "3 Les embeddings contextuels",
    "text": "3 Les embeddings contextuels\nLe principal probl√®me des embeddings classiques est que leur repr√©sentation est fixe : un m√™me token poss√®de toujours le m√™me vecteur num√©rique, quel que soit le contexte. Or, dans de nombreuses langues, notamment en fran√ßais, un m√™me mot peut avoir plusieurs significations selon son usage. De plus, des notions diff√©rentes peuvent s‚Äô√©crire de la m√™me fa√ßon (homonymes).\nPour d√©sambigu√Øser ces cas, il est essentiel de prendre en compte le contexte dans lequel un mot appara√Æt afin de d√©terminer son sens pr√©cis.\nLes embeddings contextuels permettent d‚Äôadapter la repr√©sentation num√©rique d‚Äôun mot en fonction de son contexte. Contrairement aux approches statiques, ils tiennent compte de la position et de l‚Äôinteraction entre les mots dans une phrase.\nExemple :\n- ‚ÄúLa baleine nage dans l‚Äôoc√©an.‚Äù üêã ‚Üí baleine (animal)\n- ‚ÄúJ‚Äôai un mal de baleine apr√®s ce repas.‚Äù ü§ï ‚Üí baleine (expression signifiant une douleur intense)\nLes mod√®les bas√©s sur les transformers (ex. BERT, GPT, T5) sont capables d‚Äôencoder ces diff√©rences en g√©n√©rant un vecteur unique pour chaque occurrence d‚Äôun mot selon son contexte. Ainsi, un mot aura un embedding sp√©cifique qui varie en fonction des mots qui l‚Äôentourent, tout en conservant les relations s√©mantiques apprises (synonymie, antonymie, etc.).",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#travaux-pratiques",
    "href": "4_embedding.html#travaux-pratiques",
    "title": "Du mot au vecteur",
    "section": "4 Travaux pratiques",
    "text": "4 Travaux pratiques",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "5_ressources.html",
    "href": "5_ressources.html",
    "title": "Pour aller plus loin",
    "section": "",
    "text": "Quelques ressources pour aller plus loin :\n\nCours de Lino Galiana\nCours de Julien Romero\nCours de Georges-Andr√© Silber",
    "crumbs": [
      "Ressources"
    ]
  },
  {
    "objectID": "5_ressources.html#dautres-ressources-pour-aller-plus-loin",
    "href": "5_ressources.html#dautres-ressources-pour-aller-plus-loin",
    "title": "Pour aller plus loin",
    "section": "",
    "text": "Quelques ressources pour aller plus loin :\n\nCours de Lino Galiana\nCours de Julien Romero\nCours de Georges-Andr√© Silber",
    "crumbs": [
      "Ressources"
    ]
  },
  {
    "objectID": "5_topic_modeling.html",
    "href": "5_topic_modeling.html",
    "title": "Mod√©lisation de sujets",
    "section": "",
    "text": "Le Topic Modeling est une technique de traitement du langage naturel (NLP) permettant d‚Äôextraire automatiquement des sujets latents √† partir d‚Äôun corpus de documents. Il repose sur des mod√®les probabilistes qui tentent de d√©couvrir des structures sous-jacentes dans un ensemble de textes sans supervision.\n\n\n\nLe concept de mod√©lisation de sujets a √©merg√© dans les ann√©es 1990 avec l‚Äôav√®nement des mod√®les probabilistes de classification de texte. Parmi les premi√®res approches, on retrouve Latent Semantic Analysis (LSA) introduite en 1988 et le Latent Dirichlet Allocation (LDA) propos√© par Blei, Ng et Jordan en 2003.\n\n\n\nEn fran√ßais, le terme topic peut √™tre traduit par sujet ou th√®me. Ces termes sont souvent utilis√©s de mani√®re interchangeable dans le contexte de la mod√©lisation de sujets.",
    "crumbs": [
      "Topic Modeling - LDA"
    ]
  },
  {
    "objectID": "5_topic_modeling.html#topic-modeling",
    "href": "5_topic_modeling.html#topic-modeling",
    "title": "Mod√©lisation de sujets",
    "section": "",
    "text": "Le Topic Modeling est une technique de traitement du langage naturel (NLP) permettant d‚Äôextraire automatiquement des sujets latents √† partir d‚Äôun corpus de documents. Il repose sur des mod√®les probabilistes qui tentent de d√©couvrir des structures sous-jacentes dans un ensemble de textes sans supervision.\n\n\n\nLe concept de mod√©lisation de sujets a √©merg√© dans les ann√©es 1990 avec l‚Äôav√®nement des mod√®les probabilistes de classification de texte. Parmi les premi√®res approches, on retrouve Latent Semantic Analysis (LSA) introduite en 1988 et le Latent Dirichlet Allocation (LDA) propos√© par Blei, Ng et Jordan en 2003.\n\n\n\nEn fran√ßais, le terme topic peut √™tre traduit par sujet ou th√®me. Ces termes sont souvent utilis√©s de mani√®re interchangeable dans le contexte de la mod√©lisation de sujets.",
    "crumbs": [
      "Topic Modeling - LDA"
    ]
  },
  {
    "objectID": "5_topic_modeling.html#lda-latent-dirichlet-allocation",
    "href": "5_topic_modeling.html#lda-latent-dirichlet-allocation",
    "title": "Mod√©lisation de sujets",
    "section": "2 LDA (Latent Dirichlet Allocation)",
    "text": "2 LDA (Latent Dirichlet Allocation)\n\n2.1 D√©finition du LDA\nLe Latent Dirichlet Allocation (LDA) est un mod√®le g√©n√©ratif probabiliste qui repr√©sente chaque document comme une distribution de sujets et chaque sujet comme une distribution de mots. Il suppose que chaque document est g√©n√©r√© par un m√©lange de plusieurs sujets, chacun contribuant avec une certaine probabilit√©.\n\n\n2.2 Lien vers article\nPour plus de d√©tails, vous pouvez consulter l‚Äôarticle original de Blei, Ng et Jordan (2003) : Latent Dirichlet Allocation\n\n\n2.3 LDA - G√©n√©ration de documents\nL‚Äôalgorithme de g√©n√©ration de documents sous LDA suit les √©tapes suivantes :\n\nChoisir le nombre de mots √† g√©n√©rer pour le document (N)\nChoisir la distribution des sujets pour un document selon une distribution de Dirichlet. (\\(\\theta\\))\nPour chaque mot du document :\n\nS√©lectionner un sujet selon la distribution de sujets du document.\nS√©lectionner un mot selon la distribution de mots associ√©e √† ce sujet.\n\n\nCela permet de g√©n√©rer un document synth√©tique bas√© sur un mod√®le probabiliste des sujets et des mots.\nSachant une distribution de topics, comment construire un document ?\n\n2.3.1 Exemple √† deux sujets\n\nsujet_1 = CET; sujet_2 = t√©l√©travail Soit la distribution suivante:\n\n\nsujet_1_distribution = {‚Äúcompte‚Äù:0.16,‚ÄúCET‚Äù:0.5,‚Äú√©pargne‚Äù:0.16,‚Äútemps‚Äù:0.16}\nsujet_2_distribution = {‚Äút√©l√©travail‚Äù:0.7,‚Äúdroit‚Äù:0.15,‚Äúd√©connexion‚Äù:0.15}\n\n\nG√©n√©ration de 4 mots (N=4)\n\nG√©n√©ration de la distribution en sujet du document (admettons sujet_1=3/4, sujet_2=1/4) Pour le premier mot :\n\nselection d‚Äôun sujet selon la distribution 3/4 ; 1/4 =&gt; sujet_1\nparmi la distribution du sujet_1 ={‚Äúcompte‚Äù:0.16,‚ÄúCET‚Äù:0.5,‚Äú√©pargne‚Äù:0.16,‚Äútemps‚Äù:0.16}, on tire un mot =&gt; ‚ÄúCET‚Äù\n\n\nphrase g√©n√©r√©e = CET\n\nPour le deuxi√®me mot : * selection d‚Äôun sujet selon la distribution 3/4 ; 1/4 =&gt; sujet_1 * parmi la distribution du sujet_1 ={‚Äúcompte‚Äù:0.16,‚ÄúCET‚Äù:0.5,‚Äú√©pargne‚Äù:0.16,‚Äútemps‚Äù:0.16}, on tire un mot =&gt; ‚Äúcompte‚Äù\n\nphrase g√©n√©r√©e = CET compte\n\netc.\n\n\n\n2.4 LDA - G√©n√©ration de topics\nL‚Äôalgorithme de g√©n√©ration de topics sous LDA repose sur une approche de type inf√©rence bay√©sienne :\n\nInitialiser al√©atoirement une attribution de sujets aux mots du corpus.\nMettre √† jour ces attributions en fonction de la probabilit√© conditionnelle d‚Äôun mot appartenant √† un sujet donn√©, bas√©e sur les occurrences dans le corpus.\nR√©p√©ter l‚Äôop√©ration jusqu‚Äô√† convergence.\n\nDeux techniques principales sont utilis√©es pour l‚Äôinf√©rence :\n\nGibbs Sampling, une m√©thode de Monte Carlo par cha√Ænes de Markov (MCMC).\nVariationnal Bayes, une approximation de l‚Äôinf√©rence exacte.\n\nCe processus permet d‚Äôextraire des distributions de sujets et de mots qui d√©finissent les th√©matiques sous-jacentes du corpus √©tudi√©.\nSachant des documents observ√©s, comment retrouver/construire les topics ?\n\n2.4.1 Exemple √† deux sujets avec trois documents\n\n2.4.1.1 Initialisation\nCorpus de trois documents :\n\nDocument_1 = ‚ÄúLe t√©l√©travail est un droit‚Äù\nDocument_2 = ‚ÄúLe droit √† la deconnexion est un droit‚Äù\nDocument_3 = ‚ÄúLe plafond annuel du CET est de 40 jours‚Äù\n\npour l‚Äôexemple, travaillons en concept :\n\nDocument_1 = ‚Äút√©l√©travail √™tre droit‚Äù\nDocument_2 = ‚Äúdroit deconnexion √™tre droit‚Äù\nDocument_3 = ‚Äúplafond annuel CET √™tre 40 jours‚Äù\n\n\n\n2.4.1.2 Calcul de probabilit√©s\nk=2 sujets, affectons al√©atoirement chacun des concepts de chaque document √† un sujet\n\nDocument_1 = ‚Äút√©l√©travail(2) √™tre(1) droit(1)‚Äù\nDocument_2 = ‚Äúdroit(2) deconnexion(1) √™tre(2) droit(2)‚Äù\nDocument_3 = ‚Äúplafond(1) annuel(1) CET(2) √™tre(1) 40(1) jours(2)‚Äù\n\nCeci donne la r√©partition suivante (ces contours vont bouger √† chaque √©tape!) :\n\n\\(sujet_1=\\{√™tre,droit,deconnexion,plafond,annuel,√™tre,40\\}\\) ; \\(sujet_2=\\{t√©l√©travail, droit, √™tre, droit, CET, jours\\}\\)\n\non remarque le mot ‚Äúdroit‚Äù est dans les deux sujets, on peut calculer des probabilit√©s empiriques conditionnelles diff√©rentes [Repr√©sentativit√© du mot dans un sujet ou poids] * \\(P(mot='droit'| 'droit' appartient au sujet_1)=1/7\\) et \\(P(mot='droit'| 'droit' appartient au sujet_2)=2/6\\)\nOn peut aussi calcul des probabilit√©s conditionnelles empiriques sachant les documents * \\(P(mot='droit'| document=document_1)=1/3\\) et \\(P(mot='droit'| document=document_2)=2/4\\)\nEt m√™me comment les sujets sont repr√©sent√©s dans les documents [R√©presentativit√© des sujets dans un document]: * \\(P(sujet=sujet_1 | document=document_1)=2/3\\) et \\(P(sujet=sujet_1 | document=document_2)=1/4\\)\n\n\n2.4.1.3 R√©√©valuation et bis repetita\nPour chaque mot, nous r√©√©valuons sont appartenance √† un sujet. Il faut alors calculer les nouvelles probabilit√©s conditionnelles que le mot appartiennent au sujet i\n\n\\(P(\"droit\" appartient au sujet_1 | mot=\"droit\" ) \\alpha P(mot=\"droit\"| \"droit\" appartient au sujet_1) * P(sujet=sujet_1 | document=document_1)\\)\non calcule pour les autres sujets et on affecte au plus probable\non arr√™te l‚Äôalgorithme quand il converge",
    "crumbs": [
      "Topic Modeling - LDA"
    ]
  },
  {
    "objectID": "5_topic_modeling.html#travaux-pratiques",
    "href": "5_topic_modeling.html#travaux-pratiques",
    "title": "Mod√©lisation de sujets",
    "section": "3 Travaux pratiques",
    "text": "3 Travaux pratiques",
    "crumbs": [
      "Topic Modeling - LDA"
    ]
  },
  {
    "objectID": "7_RAG.html",
    "href": "7_RAG.html",
    "title": "LLM et RAG",
    "section": "",
    "text": "Les Large Language Models (LLM) sont des r√©seaux de neurones de tr√®s grande taille, comptant souvent plusieurs milliards de param√®tres, √©galement appel√©s poids (correspondant √† ceux des neurones). Ces mod√®les sont entra√Æn√©s sur d‚Äôimmenses volumes de donn√©es, incluant une grande partie du contenu disponible sur Internet. Ils s‚Äôappuient principalement sur l‚Äôarchitecture des Transformers, une structure sp√©cifique de r√©seau de neurones compos√©e de plusieurs modules empil√©s.\nL‚Äôobjectif des Transformers est d‚Äôapprendre √† pr√©dire les tokens (unit√©s de texte) de mani√®re conditionnelle, c‚Äôest-√†-dire en fonction des tokens pr√©c√©dents. Cette architecture surpasse les mod√®les bidirectionnels traditionnels, notamment gr√¢ce √† l‚Äôintroduction des t√™tes d‚Äôattention. Ces m√©canismes permettent au mod√®le de concentrer plus ou moins d‚Äôattention sur certains tokens en fonction du contexte. Par exemple, une t√™te d‚Äôattention peut se sp√©cialiser dans la structure grammaticale d‚Äôune phrase, en mettant l‚Äôaccent sur la relation entre le sujet, le verbe et le compl√©ment (les tokens correspondant auront alors un poids plus important que les stopwords par exemple).\ne.g : GPT, Gemini, Claude, Llama, Mistral",
    "crumbs": [
      "LLM et RAG"
    ]
  },
  {
    "objectID": "7_RAG.html#llm",
    "href": "7_RAG.html#llm",
    "title": "LLM et RAG",
    "section": "",
    "text": "Les Large Language Models (LLM) sont des r√©seaux de neurones de tr√®s grande taille, comptant souvent plusieurs milliards de param√®tres, √©galement appel√©s poids (correspondant √† ceux des neurones). Ces mod√®les sont entra√Æn√©s sur d‚Äôimmenses volumes de donn√©es, incluant une grande partie du contenu disponible sur Internet. Ils s‚Äôappuient principalement sur l‚Äôarchitecture des Transformers, une structure sp√©cifique de r√©seau de neurones compos√©e de plusieurs modules empil√©s.\nL‚Äôobjectif des Transformers est d‚Äôapprendre √† pr√©dire les tokens (unit√©s de texte) de mani√®re conditionnelle, c‚Äôest-√†-dire en fonction des tokens pr√©c√©dents. Cette architecture surpasse les mod√®les bidirectionnels traditionnels, notamment gr√¢ce √† l‚Äôintroduction des t√™tes d‚Äôattention. Ces m√©canismes permettent au mod√®le de concentrer plus ou moins d‚Äôattention sur certains tokens en fonction du contexte. Par exemple, une t√™te d‚Äôattention peut se sp√©cialiser dans la structure grammaticale d‚Äôune phrase, en mettant l‚Äôaccent sur la relation entre le sujet, le verbe et le compl√©ment (les tokens correspondant auront alors un poids plus important que les stopwords par exemple).\ne.g : GPT, Gemini, Claude, Llama, Mistral",
    "crumbs": [
      "LLM et RAG"
    ]
  },
  {
    "objectID": "7_RAG.html#rag",
    "href": "7_RAG.html#rag",
    "title": "LLM et RAG",
    "section": "2 RAG",
    "text": "2 RAG\nLes LLM (Large Language Models) disposent d‚Äôune capacit√© limit√©e ‚Äî bien que cons√©quente ‚Äî √† traiter des donn√©es en entr√©e, appel√©e taille d‚Äôinput (input_size). Cette contrainte signifie que certains documents, trop volumineux, ne peuvent pas √™tre analys√©s en une seule fois. Par ailleurs, m√™me lorsqu‚Äôun document entre enti√®rement dans cette capacit√©, il est souvent inutile d‚Äôen exploiter l‚Äôensemble pour accomplir une t√¢che sp√©cifique, comme r√©pondre √† une question. Limiter la quantit√© de texte trait√© permet √©galement de r√©duire le co√ªt computationnel, puisque plus le nombre de tokens est √©lev√©, plus les calculs sont complexes et on√©reux.\nL‚Äôarchitecture Retrieval-Augmented Generation (RAG) apporte une solution efficace √† ce probl√®me. Elle consiste √† d√©couper un document en segments plus petits, appel√©s chunks, et √† ne transmettre au mod√®le que les parties les plus pertinentes pour la t√¢che demand√©e.\nLe processus se d√©roule g√©n√©ralement en quatre √©tapes :\n\nD√©coupage et vectorisation (Embedding) : Le document est segment√© selon diff√©rentes strat√©gies (par paragraphe, tous les X caract√®res, ou via un s√©parateur sp√©cifique). Chaque segment est ensuite transform√© en vecteur dans un espace vectoriel √† l‚Äôaide d‚Äôun embedder.\nRecherche des segments pertinents (Retrieval) : Lorsqu‚Äôune question est pos√©e, elle est √©galement convertie en vecteur, puis compar√©e aux vecteurs des segments. On s√©lectionne g√©n√©ralement les 5 √† 10 chunks les plus proches.\nConstruction de la question augment√©e (Augmentation) : La question est enrichie avec les chunks les plus pertinents afin de fournir au mod√®le le contexte utile.\nG√©n√©ration de la r√©ponse (Generation) : Le LLM produit une r√©ponse en s‚Äôappuyant sur la question augment√©e et les √©l√©ments de contexte fournis.",
    "crumbs": [
      "LLM et RAG"
    ]
  },
  {
    "objectID": "7_RAG.html#le-rag-en-pratique",
    "href": "7_RAG.html#le-rag-en-pratique",
    "title": "LLM et RAG",
    "section": "3 le RAG en pratique",
    "text": "3 le RAG en pratique\nLa mise en place d‚Äôune architecture Retrieval-Augmented Generation (RAG) n√©cessite de choisir les bons outils et de d√©finir un pipeline clair.\nTout d‚Äôabord, pour impl√©menter le cha√Ænage des diff√©rentes √©tapes, deux frameworks populaires s‚Äôoffrent √† vous : Langchain et LlamaIndex. Langchain est tr√®s flexible et permet de cr√©er des cha√Ænes complexes, parfait pour les projets n√©cessitant une personnalisation fine. En revanche, LlamaIndex est plus simple, con√ßu pour g√©rer des donn√©es structur√©es et non structur√©es, id√©al pour des t√¢ches de r√©cup√©ration de documents.\nEnsuite, il faut choisir une base de donn√©es vectorielle pour stocker et r√©cup√©rer les vecteurs d‚Äôembarquement. Parmi les options disponibles, FAISS est rapide et l√©ger, parfait pour des d√©ploiements locaux ou des projets open-source. Qdrant est plus robuste, adapt√© √† une utilisation √† grande √©chelle, avec une API facile √† int√©grer. Enfin, ChromaDB est int√©gr√© √† Langchain et convient bien pour des prototypes ou des projets de taille moyenne.\nUne fois les outils choisis, il convient de d√©finir l‚Äôarchitecture. Le processus commence par le d√©coupage du document en chunks, qui sont ensuite convertis en vecteurs via un embedder. Ces vecteurs sont stock√©s dans la base vectorielle. Lorsque l‚Äôutilisateur pose une question, elle est transform√©e en vecteur et compar√©e aux vecteurs stock√©s pour retrouver les chunks les plus pertinents. Ces derniers sont combin√©s avec la question pour cr√©er une question augment√©e. Enfin, cette question enrichie est envoy√©e √† un LLM (comme GPT-4 ou Claude) pour g√©n√©rer une r√©ponse.\nCette architecture optimise l‚Äôefficacit√© du mod√®le en ne fournissant que les informations pertinentes, ce qui r√©duit le co√ªt computationnel et am√©liore la qualit√© des r√©ponses. Elle est √©galement flexible, permettant de personnaliser chaque √©tape du processus en fonction des besoins sp√©cifiques du projet.",
    "crumbs": [
      "LLM et RAG"
    ]
  },
  {
    "objectID": "7_RAG.html#travaux-pratiques",
    "href": "7_RAG.html#travaux-pratiques",
    "title": "LLM et RAG",
    "section": "4 Travaux pratiques",
    "text": "4 Travaux pratiques",
    "crumbs": [
      "LLM et RAG"
    ]
  },
  {
    "objectID": "2_tokenization.html",
    "href": "2_tokenization.html",
    "title": "Tokenization : premi√®re √©tape du NLP",
    "section": "",
    "text": "La tokenisation est le processus de d√©coupage d‚Äôun long √©l√©ment en plusieurs petits √©l√©ments. Elle est essentielle en NLP (traitement du langage naturel) afin de fournir progressivement des fragments d‚Äôinformation √† la machine. Quel que soit sa longueur, un texte doit √™tre segment√© en petits morceaux pour √™tre trait√© s√©quentiellement par l‚Äôalgorithme.\nLa tokenisation constitue la base du NLP et influence fortement les r√©sultats en fonction des choix effectu√©s. Traditionnellement, elle s‚Äôeffectue au niveau des mots ou des sous-mots : une phrase enti√®re est trop volumineuse pour √™tre comprise par la machine, tandis que les caract√®res pris individuellement sont trop atomiques pour les langues latines et cyrilliques.\nExemples de tokenisation :\n\nD√©coupage en tokens mots :\n‚ÄúLa tokenisation en NLP est primordiale.‚Äù ‚Üí \"La\", \"tokenisation\", \"en\", \"NLP\", \"est\", \"primordiale\"\n\nD√©coupage en tokens sous-mots :\n‚ÄúLa tokenisation en NLP est primordiale.‚Äù ‚Üí \"La\", \"token\", \"isation\", \"en\", \"NLP\", \"est\", \"prim\", \"ordiale\"\n\n\n\n\n\n\nUne fois la tokenisation r√©alis√©e, on peut analyser les occurrences des tokens et observer des r√©gularit√©s statistiques, telles que :\n- Les mots les plus fr√©quents (top mots).\n- Les mots qui apparaissent souvent ensemble, formant des expressions courantes.\n- Les mots qui reviennent dans des contextes similaires.\nEn NLP, on formule l‚Äôhypoth√®se distributionnelle suivante : les mots qui apparaissent souvent dans des contextes similaires sont proches et a priori ont de fortes chances d‚Äôavoir le m√™me sens.\n\n\n\nLa machine ne comprend que des suites de 0 et 1. Il est donc n√©cessaire de convertir chaque token en une repr√©sentation num√©rique. Pour ce faire, on lui attribue un vecteur num√©rique de taille n (suffisamment grand, mais pas excessif).\nUne premi√®re approche, appel√©e one-hot encoding, consiste √† associer √† chaque token un vecteur canonique ( e_n ). Cependant, cette repr√©sentation est orthogonale et ne refl√®te pas bien la structure du langage :\n- Les tokens correspondant √† des synonymes sont tout aussi distants que des antonymes, alors qu‚Äôon souhaiterait au contraire mod√©liser leur proximit√© s√©mantique.\n- Une meilleure approche consisterait √† int√©grer ces relations s√©mantiques en utilisant des repr√©sentations vectorielles plus avanc√©es, comme les word embeddings (ex. Word2Vec, GloVe, BERT).\n\n\n\n\nPlusieurs librairies, en g√©n√©ral en python, permettent de tokenizer un texte :\n\nNLTK\nSpacy\nHuggingFace",
    "crumbs": [
      "Qu'est ce que la tokenization ?"
    ]
  },
  {
    "objectID": "2_tokenization.html#quest-ce-que-la-tokenization",
    "href": "2_tokenization.html#quest-ce-que-la-tokenization",
    "title": "Tokenization : premi√®re √©tape du NLP",
    "section": "",
    "text": "La tokenisation est le processus de d√©coupage d‚Äôun long √©l√©ment en plusieurs petits √©l√©ments. Elle est essentielle en NLP (traitement du langage naturel) afin de fournir progressivement des fragments d‚Äôinformation √† la machine. Quel que soit sa longueur, un texte doit √™tre segment√© en petits morceaux pour √™tre trait√© s√©quentiellement par l‚Äôalgorithme.\nLa tokenisation constitue la base du NLP et influence fortement les r√©sultats en fonction des choix effectu√©s. Traditionnellement, elle s‚Äôeffectue au niveau des mots ou des sous-mots : une phrase enti√®re est trop volumineuse pour √™tre comprise par la machine, tandis que les caract√®res pris individuellement sont trop atomiques pour les langues latines et cyrilliques.\nExemples de tokenisation :\n\nD√©coupage en tokens mots :\n‚ÄúLa tokenisation en NLP est primordiale.‚Äù ‚Üí \"La\", \"tokenisation\", \"en\", \"NLP\", \"est\", \"primordiale\"\n\nD√©coupage en tokens sous-mots :\n‚ÄúLa tokenisation en NLP est primordiale.‚Äù ‚Üí \"La\", \"token\", \"isation\", \"en\", \"NLP\", \"est\", \"prim\", \"ordiale\"\n\n\n\n\n\n\nUne fois la tokenisation r√©alis√©e, on peut analyser les occurrences des tokens et observer des r√©gularit√©s statistiques, telles que :\n- Les mots les plus fr√©quents (top mots).\n- Les mots qui apparaissent souvent ensemble, formant des expressions courantes.\n- Les mots qui reviennent dans des contextes similaires.\nEn NLP, on formule l‚Äôhypoth√®se distributionnelle suivante : les mots qui apparaissent souvent dans des contextes similaires sont proches et a priori ont de fortes chances d‚Äôavoir le m√™me sens.\n\n\n\nLa machine ne comprend que des suites de 0 et 1. Il est donc n√©cessaire de convertir chaque token en une repr√©sentation num√©rique. Pour ce faire, on lui attribue un vecteur num√©rique de taille n (suffisamment grand, mais pas excessif).\nUne premi√®re approche, appel√©e one-hot encoding, consiste √† associer √† chaque token un vecteur canonique ( e_n ). Cependant, cette repr√©sentation est orthogonale et ne refl√®te pas bien la structure du langage :\n- Les tokens correspondant √† des synonymes sont tout aussi distants que des antonymes, alors qu‚Äôon souhaiterait au contraire mod√©liser leur proximit√© s√©mantique.\n- Une meilleure approche consisterait √† int√©grer ces relations s√©mantiques en utilisant des repr√©sentations vectorielles plus avanc√©es, comme les word embeddings (ex. Word2Vec, GloVe, BERT).\n\n\n\n\nPlusieurs librairies, en g√©n√©ral en python, permettent de tokenizer un texte :\n\nNLTK\nSpacy\nHuggingFace",
    "crumbs": [
      "Qu'est ce que la tokenization ?"
    ]
  }
]
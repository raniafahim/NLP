[
  {
    "objectID": "3_preprocessing.html",
    "href": "3_preprocessing.html",
    "title": "Les Ã©tapes du pre-processing",
    "section": "",
    "text": "Le prÃ©processing consiste Ã  nettoyer et prÃ©parer un texte avant son traitement par des algorithmes de machine learning. Il permet dâ€™amÃ©liorer la qualitÃ© des donnÃ©es et dâ€™optimiser les performances des modÃ¨les de NLP (traitement du langage naturel).\n\n\n\nLa tokenisation fait partie du prÃ©processing. Elle permet de segmenter un texte en unitÃ©s plus petites (mots, sous-mots ou caractÃ¨res) afin de faciliter lâ€™analyse. Cette Ã©tape est essentielle pour rÃ©aliser des analyses statistiques exploratoires, comme identifier le thÃ¨me dâ€™un texte ou analyser les sentiments.\n\n\n\nLa normalisation consiste Ã  uniformiser lâ€™Ã©criture dâ€™un texte afin de regrouper des tokens similaires. Cela inclut :\n- La conversion en majuscules ou minuscules pour Ã©viter de traiter â€œParisâ€ et â€œparisâ€ comme deux mots distincts.\n- La suppression des accents (ex. Ã© â†’ e) pour uniformiser les termes.\n\n\n\nLes stopwords sont des mots qui nâ€™apportent pas dâ€™information significative sur le sens du texte, mais servent uniquement Ã  la structure syntaxique. Il sâ€™agit notamment des articles (le, la, un, des), des prÃ©positions (Ã , de, pour), ou encore des conjonctions (et, mais, donc). Leur suppression permet de rÃ©duire le bruit et dâ€™amÃ©liorer lâ€™efficacitÃ© du modÃ¨le.\n\n\n\n\nLa lemmatisation consiste Ã  rÃ©duire un mot Ã  sa forme canonique (ou lemme), câ€™est-Ã -dire sa version trouvÃ©e dans le dictionnaire.\n\nEx. â€œmangÃ©â€, â€œmangeonsâ€, â€œmangeaientâ€ â†’ â€œmangerâ€\n\n\nLa stemmatisation, plus rudimentaire, tronque les mots Ã  leur racine commune sans tenir compte des rÃ¨gles linguistiques.\n\nEx. â€œmangerâ€, â€œmangÃ©â€, â€œmangeonsâ€ â†’ â€œmangâ€\n\n\nCes techniques permettent de regrouper les variantes dâ€™un mÃªme mot et dâ€™amÃ©liorer lâ€™analyse du texte.\n\n\n\nToutes ces Ã©tapes sont adaptÃ©es en fonction de la problÃ©matique. Il nâ€™existe pas de mÃ©thode unique, mais plutÃ´t des approches variÃ©es selon les besoins du projet. Il est donc essentiel de tester plusieurs stratÃ©gies afin dâ€™identifier la plus efficace.",
    "crumbs": [
      "PrÃ©traitement des donnÃ©es"
    ]
  },
  {
    "objectID": "3_preprocessing.html#les-Ã©tapes-du-pre-processing",
    "href": "3_preprocessing.html#les-Ã©tapes-du-pre-processing",
    "title": "Les Ã©tapes du pre-processing",
    "section": "",
    "text": "Le prÃ©processing consiste Ã  nettoyer et prÃ©parer un texte avant son traitement par des algorithmes de machine learning. Il permet dâ€™amÃ©liorer la qualitÃ© des donnÃ©es et dâ€™optimiser les performances des modÃ¨les de NLP (traitement du langage naturel).\n\n\n\nLa tokenisation fait partie du prÃ©processing. Elle permet de segmenter un texte en unitÃ©s plus petites (mots, sous-mots ou caractÃ¨res) afin de faciliter lâ€™analyse. Cette Ã©tape est essentielle pour rÃ©aliser des analyses statistiques exploratoires, comme identifier le thÃ¨me dâ€™un texte ou analyser les sentiments.\n\n\n\nLa normalisation consiste Ã  uniformiser lâ€™Ã©criture dâ€™un texte afin de regrouper des tokens similaires. Cela inclut :\n- La conversion en majuscules ou minuscules pour Ã©viter de traiter â€œParisâ€ et â€œparisâ€ comme deux mots distincts.\n- La suppression des accents (ex. Ã© â†’ e) pour uniformiser les termes.\n\n\n\nLes stopwords sont des mots qui nâ€™apportent pas dâ€™information significative sur le sens du texte, mais servent uniquement Ã  la structure syntaxique. Il sâ€™agit notamment des articles (le, la, un, des), des prÃ©positions (Ã , de, pour), ou encore des conjonctions (et, mais, donc). Leur suppression permet de rÃ©duire le bruit et dâ€™amÃ©liorer lâ€™efficacitÃ© du modÃ¨le.\n\n\n\n\nLa lemmatisation consiste Ã  rÃ©duire un mot Ã  sa forme canonique (ou lemme), câ€™est-Ã -dire sa version trouvÃ©e dans le dictionnaire.\n\nEx. â€œmangÃ©â€, â€œmangeonsâ€, â€œmangeaientâ€ â†’ â€œmangerâ€\n\n\nLa stemmatisation, plus rudimentaire, tronque les mots Ã  leur racine commune sans tenir compte des rÃ¨gles linguistiques.\n\nEx. â€œmangerâ€, â€œmangÃ©â€, â€œmangeonsâ€ â†’ â€œmangâ€\n\n\nCes techniques permettent de regrouper les variantes dâ€™un mÃªme mot et dâ€™amÃ©liorer lâ€™analyse du texte.\n\n\n\nToutes ces Ã©tapes sont adaptÃ©es en fonction de la problÃ©matique. Il nâ€™existe pas de mÃ©thode unique, mais plutÃ´t des approches variÃ©es selon les besoins du projet. Il est donc essentiel de tester plusieurs stratÃ©gies afin dâ€™identifier la plus efficace.",
    "crumbs": [
      "PrÃ©traitement des donnÃ©es"
    ]
  },
  {
    "objectID": "3_preprocessing.html#travaux-pratiques",
    "href": "3_preprocessing.html#travaux-pratiques",
    "title": "Les Ã©tapes du pre-processing",
    "section": "2 Travaux pratiques",
    "text": "2 Travaux pratiques",
    "crumbs": [
      "PrÃ©traitement des donnÃ©es"
    ]
  },
  {
    "objectID": "6_topic_modeling_BERTopic.html",
    "href": "6_topic_modeling_BERTopic.html",
    "title": "BERTopic",
    "section": "",
    "text": "BERTopic est une bibliothÃ¨que de Topic Modeling qui permet dâ€™utiliser des connaissances externes pour construire les topics. Elle repose sur une mÃ©thodologie en deux Ã©tapes. Dans un premier temps, BERTopic dÃ©termine le nombre de topics en projetant les documents dans un espace vectoriel (embeddings), en rÃ©duisant la dimension de cet espace, puis en appliquant un algorithme de clustering. Une fois cette Ã©tape terminÃ©e, les clusters (topics) sont formÃ©s et les documents leur sont attribuÃ©s. Dans un second temps, BERTopic caractÃ©rise les topics en analysant les documents associÃ©s, afin dâ€™identifier les mots les plus frÃ©quents spÃ©cifiquement prÃ©sents dans chaque topic."
  },
  {
    "objectID": "6_topic_modeling_BERTopic.html#bertopic",
    "href": "6_topic_modeling_BERTopic.html#bertopic",
    "title": "BERTopic",
    "section": "",
    "text": "BERTopic est une bibliothÃ¨que de Topic Modeling qui permet dâ€™utiliser des connaissances externes pour construire les topics. Elle repose sur une mÃ©thodologie en deux Ã©tapes. Dans un premier temps, BERTopic dÃ©termine le nombre de topics en projetant les documents dans un espace vectoriel (embeddings), en rÃ©duisant la dimension de cet espace, puis en appliquant un algorithme de clustering. Une fois cette Ã©tape terminÃ©e, les clusters (topics) sont formÃ©s et les documents leur sont attribuÃ©s. Dans un second temps, BERTopic caractÃ©rise les topics en analysant les documents associÃ©s, afin dâ€™identifier les mots les plus frÃ©quents spÃ©cifiquement prÃ©sents dans chaque topic."
  },
  {
    "objectID": "6_topic_modeling_BERTopic.html#mÃ©thodologie-bertopic",
    "href": "6_topic_modeling_BERTopic.html#mÃ©thodologie-bertopic",
    "title": "BERTopic",
    "section": "2 MÃ©thodologie BERTopic",
    "text": "2 MÃ©thodologie BERTopic\n\n2.1 1 - Plongements lexicaux : du document aux vecteurs\nLâ€™objectif est de projeter un document texte dans un espace vectoriel. Cette projection peut Ãªtre rÃ©alisÃ©e soit sur lâ€™ensemble du document, soit en le dÃ©coupant prÃ©alablement en plusieurs sous-documents, qui sont ensuite projetÃ©s individuellement. Dans ce cas, lâ€™analyse est menÃ©e au niveau des sous-documents. Cette approche est gÃ©nÃ©ralement privilÃ©giÃ©e lorsque les documents sont particuliÃ¨rement longs.\n\n\n2.2 2 - RÃ©duction de dimension : dâ€™un vecteur long Ã  un vecteur court\nLe but est de rÃ©duire la dimension des vecteurs issus du plongement lexical, qui produisent gÃ©nÃ©ralement des reprÃ©sentations de grande taille, souvent supÃ©rieures Ã  500 dimensions. Pour que le clustering soit efficace, il est prÃ©fÃ©rable de travailler dans des espaces de dimensions rÃ©duites, typiquement entre 30 et 100. Plusieurs techniques peuvent Ãªtre employÃ©es Ã  cet effet, comme lâ€™Analyse en Composantes Principales (ACP) ou lâ€™UMAP. Ces mÃ©thodes permettent de conserver les dimensions les plus pertinentes tout en Ã©liminant le bruit.\n\n\n2.3 3 - Clustering : crÃ©ation des topics\nAprÃ¨s la rÃ©duction de dimension, oÃ¹ seules les dimensions les plus significatives sont conservÃ©es, BERTopic procÃ¨de Ã  un regroupement des documents en appliquant un algorithme de clustering. Les documents similaires sont ainsi organisÃ©s en clusters, chacun correspondant Ã  un topic, câ€™est-Ã -dire une thÃ©matique cohÃ©rente dÃ©duite des proximitÃ©s observÃ©es dans lâ€™espace vectoriel rÃ©duit. Lâ€™algorithme de clustering utilisÃ©, le plus souvent HDBSCAN, prÃ©sente lâ€™avantage de sâ€™adapter Ã  des structures de donnÃ©es complexes, en identifiant automatiquement le nombre appropriÃ© de clusters sans paramÃ©trage prÃ©alable. Ã€ lâ€™issue de ce processus, chaque document est rattachÃ© Ã  un topic en fonction de sa proximitÃ© avec les autres Ã©lÃ©ments du cluster, ouvrant la voie Ã  lâ€™Ã©tape suivante qui consiste Ã  caractÃ©riser le contenu sÃ©mantique de chaque topic.\n\n\n2.4 4 - Analyse des tokens : caractÃ©risation des topics\nPour chaque topic, un document agrÃ©gÃ© est constituÃ© Ã  partir de lâ€™ensemble des documents qui lui sont associÃ©s. Une analyse frÃ©quentielle des tokens est ensuite rÃ©alisÃ©e. BERTopic applique une pondÃ©ration spÃ©cifique aux tokens en utilisant la mÃ©thode c-TF-IDF (class-based TF-IDF), qui Ã©value lâ€™importance dâ€™un token en fonction de sa frÃ©quence dans le topic par rapport Ã  lâ€™ensemble du corpus. Cette approche permet de mieux identifier les termes caractÃ©ristiques de chaque topic, en mettant en avant les tokens qui sont particuliÃ¨rement reprÃ©sentatifs dâ€™un groupe donnÃ©, mÃªme sâ€™ils sont peu frÃ©quents dans lâ€™ensemble des documents."
  },
  {
    "objectID": "6_topic_modeling_BERTopic.html#composition",
    "href": "6_topic_modeling_BERTopic.html#composition",
    "title": "BERTopic",
    "section": "3 Composition",
    "text": "3 Composition\nBERTopic offre une grande flexibilitÃ©, ce qui lui permet de rÃ©pondre de maniÃ¨re souple Ã  nâ€™importe quelle problÃ©matique. La librairie autorise le choix de chaque algorithme Ã  chaque Ã©tape du processus. Pour la gÃ©nÃ©ration des plongements lexicaux, il est possible dâ€™utiliser Sentence-BERT, diffÃ©rents modÃ¨les de Transformers ou encore dâ€™autres mÃ©thodes dâ€™encodage adaptÃ©es aux spÃ©cificitÃ©s du corpus. Pour la rÃ©duction de dimension, BERTopic utilise par dÃ©faut lâ€™algorithme UMAP, mais il est Ã©galement possible dâ€™opter pour dâ€™autres techniques telles que lâ€™Analyse en Composantes Principales (ACP) ou t-SNE selon les besoins analytiques.\nEn ce qui concerne lâ€™Ã©tape de clustering, BERTopic sâ€™appuie initialement sur HDBSCAN, un algorithme de clustering hiÃ©rarchique basÃ© sur la densitÃ©, mais lâ€™utilisateur peut choisir dâ€™appliquer dâ€™autres mÃ©thodes comme K-Means ou DBSCAN en fonction de ses objectifs. De plus, BERTopic propose des fonctionnalitÃ©s avancÃ©es pour guider la dÃ©couverte des topics Ã  partir de mots-clÃ©s mÃ©tier, facilitant ainsi une approche semi-supervisÃ©e, ou pour organiser les topics de maniÃ¨re hiÃ©rarchique, en crÃ©ant des structures imbriquÃ©es qui reflÃ¨tent plus finement la complexitÃ© thÃ©matique du corpus.\nCette modularitÃ© rend BERTopic particuliÃ¨rement adaptÃ© Ã  des contextes variÃ©s, quâ€™il sâ€™agisse dâ€™exploration de donnÃ©es, dâ€™analyse thÃ©matique dirigÃ©e ou de projets nÃ©cessitant une forte personnalisation des rÃ©sultats.\n\n\n\nMÃ©thodologie de BERTopic"
  },
  {
    "objectID": "6_topic_modeling_BERTopic.html#travaux-pratiques",
    "href": "6_topic_modeling_BERTopic.html#travaux-pratiques",
    "title": "BERTopic",
    "section": "4 Travaux pratiques",
    "text": "4 Travaux pratiques"
  },
  {
    "objectID": "5_topic_modeling.html",
    "href": "5_topic_modeling.html",
    "title": "ModÃ©lisation de sujets",
    "section": "",
    "text": "Le Topic Modeling est une technique de traitement du langage naturel (NLP) permettant dâ€™extraire automatiquement des sujets latents Ã  partir dâ€™un corpus de documents. Il repose sur des modÃ¨les probabilistes qui tentent de dÃ©couvrir des structures sous-jacentes dans un ensemble de textes sans supervision.\n\n\n\nLe concept de modÃ©lisation de sujets a Ã©mergÃ© dans les annÃ©es 1990 avec lâ€™avÃ¨nement des modÃ¨les probabilistes de classification de texte. Parmi les premiÃ¨res approches, on retrouve Latent Semantic Analysis (LSA) introduite en 1988 et le Latent Dirichlet Allocation (LDA) proposÃ© par Blei, Ng et Jordan en 2003.\n\n\n\nEn franÃ§ais, le terme topic peut Ãªtre traduit par sujet ou thÃ¨me. Ces termes sont souvent utilisÃ©s de maniÃ¨re interchangeable dans le contexte de la modÃ©lisation de sujets.",
    "crumbs": [
      "Topic Modeling - LDA"
    ]
  },
  {
    "objectID": "5_topic_modeling.html#topic-modeling",
    "href": "5_topic_modeling.html#topic-modeling",
    "title": "ModÃ©lisation de sujets",
    "section": "",
    "text": "Le Topic Modeling est une technique de traitement du langage naturel (NLP) permettant dâ€™extraire automatiquement des sujets latents Ã  partir dâ€™un corpus de documents. Il repose sur des modÃ¨les probabilistes qui tentent de dÃ©couvrir des structures sous-jacentes dans un ensemble de textes sans supervision.\n\n\n\nLe concept de modÃ©lisation de sujets a Ã©mergÃ© dans les annÃ©es 1990 avec lâ€™avÃ¨nement des modÃ¨les probabilistes de classification de texte. Parmi les premiÃ¨res approches, on retrouve Latent Semantic Analysis (LSA) introduite en 1988 et le Latent Dirichlet Allocation (LDA) proposÃ© par Blei, Ng et Jordan en 2003.\n\n\n\nEn franÃ§ais, le terme topic peut Ãªtre traduit par sujet ou thÃ¨me. Ces termes sont souvent utilisÃ©s de maniÃ¨re interchangeable dans le contexte de la modÃ©lisation de sujets.",
    "crumbs": [
      "Topic Modeling - LDA"
    ]
  },
  {
    "objectID": "5_topic_modeling.html#lda-latent-dirichlet-allocation",
    "href": "5_topic_modeling.html#lda-latent-dirichlet-allocation",
    "title": "ModÃ©lisation de sujets",
    "section": "2 LDA (Latent Dirichlet Allocation)",
    "text": "2 LDA (Latent Dirichlet Allocation)\n\n2.1 DÃ©finition du LDA\nLe Latent Dirichlet Allocation (LDA) est un modÃ¨le gÃ©nÃ©ratif probabiliste qui reprÃ©sente chaque document comme une distribution de sujets et chaque sujet comme une distribution de mots. Il suppose que chaque document est gÃ©nÃ©rÃ© par un mÃ©lange de plusieurs sujets, chacun contribuant avec une certaine probabilitÃ©.\n\n\n2.2 Lien vers article\nPour plus de dÃ©tails, vous pouvez consulter lâ€™article original de Blei, Ng et Jordan (2003) : Latent Dirichlet Allocation\n\n\n2.3 LDA - GÃ©nÃ©ration de documents\nLâ€™algorithme de gÃ©nÃ©ration de documents sous LDA suit les Ã©tapes suivantes :\n\nChoisir le nombre de mots Ã  gÃ©nÃ©rer pour le document (N)\nChoisir la distribution des sujets pour un document selon une distribution de Dirichlet. (\\(\\theta\\))\nPour chaque mot du document :\n\nSÃ©lectionner un sujet selon la distribution de sujets du document.\nSÃ©lectionner un mot selon la distribution de mots associÃ©e Ã  ce sujet.\n\n\nCela permet de gÃ©nÃ©rer un document synthÃ©tique basÃ© sur un modÃ¨le probabiliste des sujets et des mots.\nSachant une distribution de topics, comment construire un document ?\n\n2.3.1 Exemple Ã  deux sujets\n\nsujet_1 = CET; sujet_2 = tÃ©lÃ©travail Soit la distribution suivante:\n\n\nsujet_1_distribution = {â€œcompteâ€:0.16,â€œCETâ€:0.5,â€œÃ©pargneâ€:0.16,â€œtempsâ€:0.16}\nsujet_2_distribution = {â€œtÃ©lÃ©travailâ€:0.7,â€œdroitâ€:0.15,â€œdÃ©connexionâ€:0.15}\n\n\nGÃ©nÃ©ration de 4 mots (N=4)\n\nGÃ©nÃ©ration de la distribution en sujet du document (admettons sujet_1=3/4, sujet_2=1/4) Pour le premier mot :\n\nselection dâ€™un sujet selon la distribution 3/4 ; 1/4 =&gt; sujet_1\nparmi la distribution du sujet_1 ={â€œcompteâ€:0.16,â€œCETâ€:0.5,â€œÃ©pargneâ€:0.16,â€œtempsâ€:0.16}, on tire un mot =&gt; â€œCETâ€\n\n\nphrase gÃ©nÃ©rÃ©e = CET\n\nPour le deuxiÃ¨me mot : * selection dâ€™un sujet selon la distribution 3/4 ; 1/4 =&gt; sujet_1 * parmi la distribution du sujet_1 ={â€œcompteâ€:0.16,â€œCETâ€:0.5,â€œÃ©pargneâ€:0.16,â€œtempsâ€:0.16}, on tire un mot =&gt; â€œcompteâ€\n\nphrase gÃ©nÃ©rÃ©e = CET compte\n\netc.\n\n\n\n2.4 LDA - GÃ©nÃ©ration de topics\nLâ€™algorithme de gÃ©nÃ©ration de topics sous LDA repose sur une approche de type infÃ©rence bayÃ©sienne :\n\nInitialiser alÃ©atoirement une attribution de sujets aux mots du corpus.\nMettre Ã  jour ces attributions en fonction de la probabilitÃ© conditionnelle dâ€™un mot appartenant Ã  un sujet donnÃ©, basÃ©e sur les occurrences dans le corpus.\nRÃ©pÃ©ter lâ€™opÃ©ration jusquâ€™Ã  convergence.\n\nDeux techniques principales sont utilisÃ©es pour lâ€™infÃ©rence :\n\nGibbs Sampling, une mÃ©thode de Monte Carlo par chaÃ®nes de Markov (MCMC).\nVariationnal Bayes, une approximation de lâ€™infÃ©rence exacte.\n\nCe processus permet dâ€™extraire des distributions de sujets et de mots qui dÃ©finissent les thÃ©matiques sous-jacentes du corpus Ã©tudiÃ©.\nSachant des documents observÃ©s, comment retrouver/construire les topics ?\n\n2.4.1 Exemple Ã  deux sujets avec trois documents\n\n2.4.1.1 Initialisation\nCorpus de trois documents :\n\nDocument_1 = â€œLe tÃ©lÃ©travail est un droitâ€\nDocument_2 = â€œLe droit Ã  la deconnexion est un droitâ€\nDocument_3 = â€œLe plafond annuel du CET est de 40 joursâ€\n\npour lâ€™exemple, travaillons en concept :\n\nDocument_1 = â€œtÃ©lÃ©travail Ãªtre droitâ€\nDocument_2 = â€œdroit deconnexion Ãªtre droitâ€\nDocument_3 = â€œplafond annuel CET Ãªtre 40 joursâ€\n\n\n\n2.4.1.2 Calcul de probabilitÃ©s\nk=2 sujets, affectons alÃ©atoirement chacun des concepts de chaque document Ã  un sujet\n\nDocument_1 = â€œtÃ©lÃ©travail(2) Ãªtre(1) droit(1)â€\nDocument_2 = â€œdroit(2) deconnexion(1) Ãªtre(2) droit(2)â€\nDocument_3 = â€œplafond(1) annuel(1) CET(2) Ãªtre(1) 40(1) jours(2)â€\n\nCeci donne la rÃ©partition suivante (ces contours vont bouger Ã  chaque Ã©tape!) :\n\n\\(sujet_1=\\{Ãªtre,droit,deconnexion,plafond,annuel,Ãªtre,40\\}\\) ; \\(sujet_2=\\{tÃ©lÃ©travail, droit, Ãªtre, droit, CET, jours\\}\\)\n\non remarque le mot â€œdroitâ€ est dans les deux sujets, on peut calculer des probabilitÃ©s empiriques conditionnelles diffÃ©rentes [ReprÃ©sentativitÃ© du mot dans un sujet ou poids] * \\(P(mot='droit'| 'droit' appartient au sujet_1)=1/7\\) et \\(P(mot='droit'| 'droit' appartient au sujet_2)=2/6\\)\nOn peut aussi calcul des probabilitÃ©s conditionnelles empiriques sachant les documents * \\(P(mot='droit'| document=document_1)=1/3\\) et \\(P(mot='droit'| document=document_2)=2/4\\)\nEt mÃªme comment les sujets sont reprÃ©sentÃ©s dans les documents [RÃ©presentativitÃ© des sujets dans un document]: * \\(P(sujet=sujet_1 | document=document_1)=2/3\\) et \\(P(sujet=sujet_1 | document=document_2)=1/4\\)\n\n\n2.4.1.3 RÃ©Ã©valuation et bis repetita\nPour chaque mot, nous rÃ©Ã©valuons sont appartenance Ã  un sujet. Il faut alors calculer les nouvelles probabilitÃ©s conditionnelles que le mot appartiennent au sujet i\n\n\\(P(\"droit\" appartient au sujet_1 | mot=\"droit\" ) \\alpha P(mot=\"droit\"| \"droit\" appartient au sujet_1) * P(sujet=sujet_1 | document=document_1)\\)\non calcule pour les autres sujets et on affecte au plus probable\non arrÃªte lâ€™algorithme quand il converge",
    "crumbs": [
      "Topic Modeling - LDA"
    ]
  },
  {
    "objectID": "5_topic_modeling.html#travaux-pratiques",
    "href": "5_topic_modeling.html#travaux-pratiques",
    "title": "ModÃ©lisation de sujets",
    "section": "3 Travaux pratiques",
    "text": "3 Travaux pratiques",
    "crumbs": [
      "Topic Modeling - LDA"
    ]
  },
  {
    "objectID": "5_ressources.html",
    "href": "5_ressources.html",
    "title": "Pour aller plus loin",
    "section": "",
    "text": "Quelques ressources pour aller plus loin :\n\nCours de Lino Galiana\nCours de Julien Romero\nCours de Georges-AndrÃ© Silber",
    "crumbs": [
      "Ressources"
    ]
  },
  {
    "objectID": "5_ressources.html#dautres-ressources-pour-aller-plus-loin",
    "href": "5_ressources.html#dautres-ressources-pour-aller-plus-loin",
    "title": "Pour aller plus loin",
    "section": "",
    "text": "Quelques ressources pour aller plus loin :\n\nCours de Lino Galiana\nCours de Julien Romero\nCours de Georges-AndrÃ© Silber",
    "crumbs": [
      "Ressources"
    ]
  },
  {
    "objectID": "4_embedding.html",
    "href": "4_embedding.html",
    "title": "Du mot au vecteur",
    "section": "",
    "text": "Un embedding est un espace vectoriel permettant dâ€™attribuer une reprÃ©sentation numÃ©rique Ã  un token. GrÃ¢ce Ã  ces reprÃ©sentations, la machine peut effectuer des opÃ©rations arithmÃ©tiques pour manipuler le langage et dÃ©terminer si des mots sont proches ou non sur le plan sÃ©mantique.\n\n\n\nAvec un vocabulaire de tokens de taille ( n ), on peut crÃ©er un espace vectoriel de dimension ( n ), oÃ¹ chaque token est reprÃ©sentÃ© par un vecteur unitaire canonique (one-hot encoding). Cependant, cette approche prÃ©sente plusieurs inconvÃ©nients :\n1. Lâ€™espace vectoriel devient trÃ¨s grand lorsque ( n ) est important.\n2. Les reprÃ©sentations sont orthogonales, ce qui signifie quâ€™elles ne capturent aucune relation sÃ©mantique entre les mots.\n\n\n\nEn sâ€™appuyant sur lâ€™hypothÃ¨se distributionnelle et les rÃ©seaux de neurones, les modÃ¨les neuronaux Bag of Words (CBOW et Skip-Gram) permettent dâ€™apprendre des reprÃ©sentations vectorielles compressÃ©es tout en conservant les proximitÃ©s et distances sÃ©mantiques entre les tokens.",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#embedding-passer-du-token-au-vecteur-numÃ©rique",
    "href": "4_embedding.html#embedding-passer-du-token-au-vecteur-numÃ©rique",
    "title": "Du mot au vecteur",
    "section": "",
    "text": "Un embedding est un espace vectoriel permettant dâ€™attribuer une reprÃ©sentation numÃ©rique Ã  un token. GrÃ¢ce Ã  ces reprÃ©sentations, la machine peut effectuer des opÃ©rations arithmÃ©tiques pour manipuler le langage et dÃ©terminer si des mots sont proches ou non sur le plan sÃ©mantique.\n\n\n\nAvec un vocabulaire de tokens de taille ( n ), on peut crÃ©er un espace vectoriel de dimension ( n ), oÃ¹ chaque token est reprÃ©sentÃ© par un vecteur unitaire canonique (one-hot encoding). Cependant, cette approche prÃ©sente plusieurs inconvÃ©nients :\n1. Lâ€™espace vectoriel devient trÃ¨s grand lorsque ( n ) est important.\n2. Les reprÃ©sentations sont orthogonales, ce qui signifie quâ€™elles ne capturent aucune relation sÃ©mantique entre les mots.\n\n\n\nEn sâ€™appuyant sur lâ€™hypothÃ¨se distributionnelle et les rÃ©seaux de neurones, les modÃ¨les neuronaux Bag of Words (CBOW et Skip-Gram) permettent dâ€™apprendre des reprÃ©sentations vectorielles compressÃ©es tout en conservant les proximitÃ©s et distances sÃ©mantiques entre les tokens.",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#les-premiers-algorithmes-sÃ©mantiques",
    "href": "4_embedding.html#les-premiers-algorithmes-sÃ©mantiques",
    "title": "Du mot au vecteur",
    "section": "2 Les premiers algorithmes sÃ©mantiques",
    "text": "2 Les premiers algorithmes sÃ©mantiques\n\n2.1 Lâ€™algorithme CBOW\nLâ€™algorithme Continuous Bag of Words (CBOW) repose sur un rÃ©seau de neurones composÃ© de deux Ã©tapes :\n1. Encodage : une couche dâ€™entrÃ©e qui compresse les mots du contexte dans un vecteur de taille rÃ©duite.\n2. DÃ©codage : une couche de sortie qui prend ce vecteur rÃ©duit et prÃ©dit le mot cible.\nLâ€™idÃ©e est dâ€™entraÃ®ner le rÃ©seau Ã  prÃ©dire un mot Ã  partir de son contexte (les mots qui lâ€™entourent).\nExemple avec un contexte de 4 mots (les deux mots avant et aprÃ¨s) :\n- Phrase : â€œLe chat dort sur le canapÃ©.â€\n- Contexte : [\"Le\", \"dort\", \"sur\", \"le\"]\n- Mot Ã  prÃ©dire : \"chat\"\n- Base dâ€™entraÃ®nement : ((â€œLeâ€, â€œdortâ€, â€œsurâ€, â€œleâ€), â€œchatâ€)\nEn parcourant un corpus, lâ€™algorithme apprend ces associations de maniÃ¨re probabiliste. Une fois lâ€™entraÃ®nement terminÃ©, on rÃ©cupÃ¨re la reprÃ©sentation vectorielle intermÃ©diaire pour associer chaque mot Ã  son embedding.\n\n\n2.2 Lâ€™algorithme Skip-Gram\nLâ€™architecture du rÃ©seau est similaire Ã  CBOW, mais avec une approche inversÃ©e :\n- PlutÃ´t que de prÃ©dire un mot Ã  partir de son contexte, Skip-Gram prÃ©dit les mots du contexte Ã  partir dâ€™un mot donnÃ©.\n- On gÃ©nÃ¨re des paires (mot, mot_contexte) en fonction de la fenÃªtre de contexte choisie.\nExemple :\n- Phrase : â€œLe chat dort sur le canapÃ©.â€\n- Mot donnÃ© : \"chat\"\n- Contexte Ã  prÃ©dire : [\"Le\", \"dort\", \"sur\", \"le\"]\n- Base dâ€™entraÃ®nement : (â€œLeâ€,â€œchatâ€) ; (â€œdortâ€,â€œchatâ€) ; (â€œsurâ€,â€œchatâ€) ; (â€œleâ€,â€œchatâ€)\nUne fois la distribution apprise, on dÃ©branche la deuxiÃ¨me couche et on utilise la rÃ©prÃ©sentation vectorielle de la couche cachÃ©e.\n\n\n2.3 Lâ€™algorithme GloVe\nLâ€™algorithme GloVe (Global Vectors for Word Representation) adopte une approche diffÃ©rente :\n- PlutÃ´t que dâ€™analyser des contextes locaux (comme CBOW et Skip-Gram), GloVe apprend des cooccurrences de mots Ã  partir dâ€™un large corpus.\n- Il construit une matrice de cooccurrence indiquant Ã  quelle frÃ©quence deux mots apparaissent ensemble dans les mÃªmes phrases.\n- Ensuite, un facteur de dÃ©composition est utilisÃ© pour gÃ©nÃ©rer des reprÃ©sentations vectorielles capturant les relations sÃ©mantiques.\nGloVe est particuliÃ¨rement efficace pour reprÃ©senter des mots ayant des relations sÃ©mantiques globales, comme :\n- Roi â€“ Reine,\n- France â€“ Paris,\n- Banque â€“ Argent.\n\n\n2.4 Lâ€™algorithme FastText\nFastText est une amÃ©lioration de CBOW et Skip-Gram :\n- Au lieu de reprÃ©senter un mot en entier, il le dÃ©compose en sous-mots (n-grams).\n- Cela permet de mieux gÃ©rer les mots rares ou inconnus en gÃ©nÃ©rant des embeddings dynamiques.\nExemple :\n- Le mot â€œapprentissageâ€ peut Ãªtre dÃ©composÃ© en [\"app\", \"pren\", \"tiss\", \"age\"].\n- Si un mot jamais vu auparavant est rencontrÃ©, son embedding peut Ãªtre infÃ©rÃ© Ã  partir de ses sous-mots.\nFastText est particuliÃ¨rement utile pour les langues morphologiquement riches (comme lâ€™allemand ou le turc) et pour gÃ©rer les fautes dâ€™orthographe ou les variantes linguistiques.",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#les-embeddings-contextuels",
    "href": "4_embedding.html#les-embeddings-contextuels",
    "title": "Du mot au vecteur",
    "section": "3 Les embeddings contextuels",
    "text": "3 Les embeddings contextuels\nLe principal problÃ¨me des embeddings classiques est que leur reprÃ©sentation est fixe : un mÃªme token possÃ¨de toujours le mÃªme vecteur numÃ©rique, quel que soit le contexte. Or, dans de nombreuses langues, notamment en franÃ§ais, un mÃªme mot peut avoir plusieurs significations selon son usage. De plus, des notions diffÃ©rentes peuvent sâ€™Ã©crire de la mÃªme faÃ§on (homonymes).\nPour dÃ©sambiguÃ¯ser ces cas, il est essentiel de prendre en compte le contexte dans lequel un mot apparaÃ®t afin de dÃ©terminer son sens prÃ©cis.\nLes embeddings contextuels permettent dâ€™adapter la reprÃ©sentation numÃ©rique dâ€™un mot en fonction de son contexte. Contrairement aux approches statiques, ils tiennent compte de la position et de lâ€™interaction entre les mots dans une phrase.\nExemple :\n- â€œLa baleine nage dans lâ€™ocÃ©an.â€ ğŸ‹ â†’ baleine (animal)\n- â€œJâ€™ai un mal de baleine aprÃ¨s ce repas.â€ ğŸ¤• â†’ baleine (expression signifiant une douleur intense)\nLes modÃ¨les basÃ©s sur les transformers (ex. BERT, GPT, T5) sont capables dâ€™encoder ces diffÃ©rences en gÃ©nÃ©rant un vecteur unique pour chaque occurrence dâ€™un mot selon son contexte. Ainsi, un mot aura un embedding spÃ©cifique qui varie en fonction des mots qui lâ€™entourent, tout en conservant les relations sÃ©mantiques apprises (synonymie, antonymie, etc.).",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "4_embedding.html#travaux-pratiques",
    "href": "4_embedding.html#travaux-pratiques",
    "title": "Du mot au vecteur",
    "section": "4 Travaux pratiques",
    "text": "4 Travaux pratiques",
    "crumbs": [
      "Word Embedding"
    ]
  },
  {
    "objectID": "1_intro.html",
    "href": "1_intro.html",
    "title": "Introduction au NLP",
    "section": "",
    "text": "Il existe plusieurs dÃ©finitions pour dÃ©finir le concept de langage naturel Lien Dictonnary .\n\nUn ensemble de mots et les systÃ¨mes de leur utilisation communs Ã  un peuple appartenant Ã  la mÃªme communautÃ© ou nation, Ã  la mÃªme zone gÃ©ographique ou Ã  la mÃªme tradition culturelle.\nLa communication par la voix de maniÃ¨re spÃ©cifiquement humaine, en utilisant des sons arbitraires de faÃ§on conventionnelle avec des significations conventionnelles.\nLe systÃ¨me de signes ou de symboles linguistiques considÃ©rÃ© dans lâ€™abstrait (opposÃ© Ã  la parole).\nTout ensemble ou systÃ¨me de tels symboles utilisÃ©s de maniÃ¨re plus ou moins uniforme par un certain nombre de personnes, leur permettant ainsi de communiquer intelligiblement entre elles.\nTout systÃ¨me de symboles formalisÃ©s, signes, sons, gestes, ou autres, utilisÃ© ou conÃ§u comme un moyen de communiquer la pensÃ©e, lâ€™Ã©motion, etc. : le langage des mathÃ©matiques ; la langue des signes. Les moyens de communication utilisÃ©s par les animaux.\n\nAinsi, on peut voir le traitement du langage naturel comme un ensemble de recherche et de dÃ©veloppement visant Ã  modÃ©liser et Ã  reproduire, Ã  lâ€™aide de machines, la capacitÃ© humaine Ã  produire et Ã  comprendre des Ã©noncÃ©s linguistiques Ã  des fins de communication. Le langage est ainsi un concept complexe et ambigu. Son interprÃ©tation sâ€™accompagne de nombreuses connaissances implicites (expressions, mÃ©taphores, mÃ©tonymiesâ€¦) et de bon sens, ce qui est trÃ¨s compliquÃ© Ã  retranscrire Ã  un ordinateur.\nExemples : Jâ€™ai lu un article sur le droit des femmes dans le journal. Jâ€™ai lu un article sur le droit des femmes dans le mÃ©tro.",
    "crumbs": [
      "Introduction au NLP"
    ]
  },
  {
    "objectID": "1_intro.html#quest-ce-que-le-traitement-du-langage-naturel",
    "href": "1_intro.html#quest-ce-que-le-traitement-du-langage-naturel",
    "title": "Introduction au NLP",
    "section": "",
    "text": "Il existe plusieurs dÃ©finitions pour dÃ©finir le concept de langage naturel Lien Dictonnary .\n\nUn ensemble de mots et les systÃ¨mes de leur utilisation communs Ã  un peuple appartenant Ã  la mÃªme communautÃ© ou nation, Ã  la mÃªme zone gÃ©ographique ou Ã  la mÃªme tradition culturelle.\nLa communication par la voix de maniÃ¨re spÃ©cifiquement humaine, en utilisant des sons arbitraires de faÃ§on conventionnelle avec des significations conventionnelles.\nLe systÃ¨me de signes ou de symboles linguistiques considÃ©rÃ© dans lâ€™abstrait (opposÃ© Ã  la parole).\nTout ensemble ou systÃ¨me de tels symboles utilisÃ©s de maniÃ¨re plus ou moins uniforme par un certain nombre de personnes, leur permettant ainsi de communiquer intelligiblement entre elles.\nTout systÃ¨me de symboles formalisÃ©s, signes, sons, gestes, ou autres, utilisÃ© ou conÃ§u comme un moyen de communiquer la pensÃ©e, lâ€™Ã©motion, etc. : le langage des mathÃ©matiques ; la langue des signes. Les moyens de communication utilisÃ©s par les animaux.\n\nAinsi, on peut voir le traitement du langage naturel comme un ensemble de recherche et de dÃ©veloppement visant Ã  modÃ©liser et Ã  reproduire, Ã  lâ€™aide de machines, la capacitÃ© humaine Ã  produire et Ã  comprendre des Ã©noncÃ©s linguistiques Ã  des fins de communication. Le langage est ainsi un concept complexe et ambigu. Son interprÃ©tation sâ€™accompagne de nombreuses connaissances implicites (expressions, mÃ©taphores, mÃ©tonymiesâ€¦) et de bon sens, ce qui est trÃ¨s compliquÃ© Ã  retranscrire Ã  un ordinateur.\nExemples : Jâ€™ai lu un article sur le droit des femmes dans le journal. Jâ€™ai lu un article sur le droit des femmes dans le mÃ©tro.",
    "crumbs": [
      "Introduction au NLP"
    ]
  },
  {
    "objectID": "2_tokenization.html",
    "href": "2_tokenization.html",
    "title": "Tokenization : premiÃ¨re Ã©tape du NLP",
    "section": "",
    "text": "La tokenisation est le processus de dÃ©coupage dâ€™un long Ã©lÃ©ment en plusieurs petits Ã©lÃ©ments. Elle est essentielle en NLP (traitement du langage naturel) afin de fournir progressivement des fragments dâ€™information Ã  la machine. Quel que soit sa longueur, un texte doit Ãªtre segmentÃ© en petits morceaux pour Ãªtre traitÃ© sÃ©quentiellement par lâ€™algorithme.\nLa tokenisation constitue la base du NLP et influence fortement les rÃ©sultats en fonction des choix effectuÃ©s. Traditionnellement, elle sâ€™effectue au niveau des mots ou des sous-mots : une phrase entiÃ¨re est trop volumineuse pour Ãªtre comprise par la machine, tandis que les caractÃ¨res pris individuellement sont trop atomiques pour les langues latines et cyrilliques.\nExemples de tokenisation :\n\nDÃ©coupage en tokens mots :\nâ€œLa tokenisation en NLP est primordiale.â€ â†’ \"La\", \"tokenisation\", \"en\", \"NLP\", \"est\", \"primordiale\"\n\nDÃ©coupage en tokens sous-mots :\nâ€œLa tokenisation en NLP est primordiale.â€ â†’ \"La\", \"token\", \"isation\", \"en\", \"NLP\", \"est\", \"prim\", \"ordiale\"\n\n\n\n\n\n\nUne fois la tokenisation rÃ©alisÃ©e, on peut analyser les occurrences des tokens et observer des rÃ©gularitÃ©s statistiques, telles que :\n- Les mots les plus frÃ©quents (top mots).\n- Les mots qui apparaissent souvent ensemble, formant des expressions courantes.\n- Les mots qui reviennent dans des contextes similaires.\nEn NLP, on formule lâ€™hypothÃ¨se distributionnelle suivante : les mots qui apparaissent souvent dans des contextes similaires sont proches et a priori ont de fortes chances dâ€™avoir le mÃªme sens.\n\n\n\nLa machine ne comprend que des suites de 0 et 1. Il est donc nÃ©cessaire de convertir chaque token en une reprÃ©sentation numÃ©rique. Pour ce faire, on lui attribue un vecteur numÃ©rique de taille n (suffisamment grand, mais pas excessif).\nUne premiÃ¨re approche, appelÃ©e one-hot encoding, consiste Ã  associer Ã  chaque token un vecteur canonique ( e_n ). Cependant, cette reprÃ©sentation est orthogonale et ne reflÃ¨te pas bien la structure du langage :\n- Les tokens correspondant Ã  des synonymes sont tout aussi distants que des antonymes, alors quâ€™on souhaiterait au contraire modÃ©liser leur proximitÃ© sÃ©mantique.\n- Une meilleure approche consisterait Ã  intÃ©grer ces relations sÃ©mantiques en utilisant des reprÃ©sentations vectorielles plus avancÃ©es, comme les word embeddings (ex. Word2Vec, GloVe, BERT).\n\n\n\n\nPlusieurs librairies, en gÃ©nÃ©ral en python, permettent de tokenizer un texte :\n\nNLTK\nSpacy\nHuggingFace",
    "crumbs": [
      "Qu'est ce que la tokenization ?"
    ]
  },
  {
    "objectID": "2_tokenization.html#quest-ce-que-la-tokenization",
    "href": "2_tokenization.html#quest-ce-que-la-tokenization",
    "title": "Tokenization : premiÃ¨re Ã©tape du NLP",
    "section": "",
    "text": "La tokenisation est le processus de dÃ©coupage dâ€™un long Ã©lÃ©ment en plusieurs petits Ã©lÃ©ments. Elle est essentielle en NLP (traitement du langage naturel) afin de fournir progressivement des fragments dâ€™information Ã  la machine. Quel que soit sa longueur, un texte doit Ãªtre segmentÃ© en petits morceaux pour Ãªtre traitÃ© sÃ©quentiellement par lâ€™algorithme.\nLa tokenisation constitue la base du NLP et influence fortement les rÃ©sultats en fonction des choix effectuÃ©s. Traditionnellement, elle sâ€™effectue au niveau des mots ou des sous-mots : une phrase entiÃ¨re est trop volumineuse pour Ãªtre comprise par la machine, tandis que les caractÃ¨res pris individuellement sont trop atomiques pour les langues latines et cyrilliques.\nExemples de tokenisation :\n\nDÃ©coupage en tokens mots :\nâ€œLa tokenisation en NLP est primordiale.â€ â†’ \"La\", \"tokenisation\", \"en\", \"NLP\", \"est\", \"primordiale\"\n\nDÃ©coupage en tokens sous-mots :\nâ€œLa tokenisation en NLP est primordiale.â€ â†’ \"La\", \"token\", \"isation\", \"en\", \"NLP\", \"est\", \"prim\", \"ordiale\"\n\n\n\n\n\n\nUne fois la tokenisation rÃ©alisÃ©e, on peut analyser les occurrences des tokens et observer des rÃ©gularitÃ©s statistiques, telles que :\n- Les mots les plus frÃ©quents (top mots).\n- Les mots qui apparaissent souvent ensemble, formant des expressions courantes.\n- Les mots qui reviennent dans des contextes similaires.\nEn NLP, on formule lâ€™hypothÃ¨se distributionnelle suivante : les mots qui apparaissent souvent dans des contextes similaires sont proches et a priori ont de fortes chances dâ€™avoir le mÃªme sens.\n\n\n\nLa machine ne comprend que des suites de 0 et 1. Il est donc nÃ©cessaire de convertir chaque token en une reprÃ©sentation numÃ©rique. Pour ce faire, on lui attribue un vecteur numÃ©rique de taille n (suffisamment grand, mais pas excessif).\nUne premiÃ¨re approche, appelÃ©e one-hot encoding, consiste Ã  associer Ã  chaque token un vecteur canonique ( e_n ). Cependant, cette reprÃ©sentation est orthogonale et ne reflÃ¨te pas bien la structure du langage :\n- Les tokens correspondant Ã  des synonymes sont tout aussi distants que des antonymes, alors quâ€™on souhaiterait au contraire modÃ©liser leur proximitÃ© sÃ©mantique.\n- Une meilleure approche consisterait Ã  intÃ©grer ces relations sÃ©mantiques en utilisant des reprÃ©sentations vectorielles plus avancÃ©es, comme les word embeddings (ex. Word2Vec, GloVe, BERT).\n\n\n\n\nPlusieurs librairies, en gÃ©nÃ©ral en python, permettent de tokenizer un texte :\n\nNLTK\nSpacy\nHuggingFace",
    "crumbs": [
      "Qu'est ce que la tokenization ?"
    ]
  }
]
---
title: "Du mot au vecteur"
author: "raniafhm"
date: "2025-17-03"
format: html
---

## Embedding : Passer du token au vecteur num√©rique

### D√©finition  

Un **embedding** est un espace vectoriel permettant d‚Äôattribuer une repr√©sentation num√©rique √† un token. Gr√¢ce √† ces repr√©sentations, la machine peut effectuer des op√©rations arithm√©tiques pour manipuler le langage et d√©terminer si des mots sont proches ou non sur le plan s√©mantique.  

### L'embedding na√Øf  

Avec un vocabulaire de tokens de taille \( n \), on peut cr√©er un **espace vectoriel de dimension \( n \)**, o√π chaque token est repr√©sent√© par un **vecteur unitaire canonique** (one-hot encoding). Cependant, cette approche pr√©sente plusieurs inconv√©nients :  
1. L‚Äôespace vectoriel devient **tr√®s grand** lorsque \( n \) est important.  
2. Les repr√©sentations sont **orthogonales**, ce qui signifie qu'elles ne capturent **aucune relation s√©mantique** entre les mots.  

### Apprentissage des relations s√©mantiques  

En s‚Äôappuyant sur **l‚Äôhypoth√®se distributionnelle** et les **r√©seaux de neurones**, les mod√®les neuronaux **Bag of Words** (CBOW et Skip-Gram) permettent d‚Äôapprendre des **repr√©sentations vectorielles compress√©es** tout en conservant les **proximit√©s et distances s√©mantiques** entre les tokens.  

## Les premiers algorithmes s√©mantiques  

### L'algorithme CBOW  

L'algorithme **Continuous Bag of Words (CBOW)** repose sur un **r√©seau de neurones** compos√© de deux √©tapes :  
1. **Encodage** : une couche d‚Äôentr√©e qui compresse les mots du contexte dans un vecteur de taille r√©duite.  
2. **D√©codage** : une couche de sortie qui prend ce vecteur r√©duit et pr√©dit le mot cible.  

L‚Äôid√©e est d‚Äôentra√Æner le r√©seau √† **pr√©dire un mot** √† partir de son **contexte** (les mots qui l‚Äôentourent).  

**Exemple avec un contexte de 4 mots** (*les deux mots avant et apr√®s*) :  
- Phrase : *"Le chat dort sur le canap√©."*  
- Contexte : `["Le", "dort", "sur", "le"]`  
- Mot √† pr√©dire : `"chat"`  
- Base d'entra√Ænement : (("Le", "dort", "sur", "le"), "chat")

En parcourant un corpus, l‚Äôalgorithme apprend ces associations de mani√®re probabiliste. Une fois l‚Äôentra√Ænement termin√©, on **r√©cup√®re la repr√©sentation vectorielle interm√©diaire** pour associer chaque mot √† son embedding.  

### L'algorithme Skip-Gram  

L'architecture du r√©seau est similaire √† CBOW, mais avec une approche invers√©e :  
- Plut√¥t que de **pr√©dire un mot √† partir de son contexte**, Skip-Gram **pr√©dit les mots du contexte √† partir d‚Äôun mot donn√©**.  
- On g√©n√®re des paires (mot, mot_contexte) en fonction de la fen√™tre de contexte choisie.  

**Exemple :**  
- Phrase : *"Le chat dort sur le canap√©."*  
- Mot donn√© : `"chat"`  
- Contexte √† pr√©dire : `["Le", "dort", "sur", "le"]`  
- Base d'entra√Ænement : ("Le","chat") ; ("dort","chat") ; ("sur","chat") ; ("le","chat")

Une fois la distribution apprise, on d√©branche la deuxi√®me couche et on utilise la r√©pr√©sentation vectorielle de la couche cach√©e.

### L'algorithme GloVe  

L'algorithme **GloVe (Global Vectors for Word Representation)** adopte une approche diff√©rente :  
- Plut√¥t que d‚Äôanalyser des contextes locaux (comme CBOW et Skip-Gram), GloVe **apprend des cooccurrences de mots** √† partir d‚Äôun large corpus.  
- Il construit une **matrice de cooccurrence** indiquant √† quelle fr√©quence deux mots apparaissent ensemble dans les m√™mes phrases.  
- Ensuite, un **facteur de d√©composition** est utilis√© pour g√©n√©rer des repr√©sentations vectorielles capturant les relations s√©mantiques.  

GloVe est particuli√®rement efficace pour repr√©senter **des mots ayant des relations s√©mantiques globales**, comme :  
- *Roi ‚Äì Reine*,  
- *France ‚Äì Paris*,  
- *Banque ‚Äì Argent*.  

### L'algorithme FastText  

FastText est une am√©lioration de CBOW et Skip-Gram :  
- **Au lieu de repr√©senter un mot en entier, il le d√©compose en sous-mots (n-grams).**  
- Cela permet de mieux g√©rer les **mots rares ou inconnus** en g√©n√©rant des embeddings dynamiques.  

**Exemple :**  
- Le mot *"apprentissage"* peut √™tre d√©compos√© en `["app", "pren", "tiss", "age"]`.  
- Si un mot jamais vu auparavant est rencontr√©, son embedding peut √™tre inf√©r√© √† partir de ses sous-mots.  

FastText est particuli√®rement utile pour **les langues morphologiquement riches** (comme l‚Äôallemand ou le turc) et pour g√©rer les **fautes d‚Äôorthographe ou les variantes linguistiques**.  


## Les embeddings contextuels

Le principal probl√®me des embeddings classiques est que leur **repr√©sentation est fixe** : un m√™me token poss√®de toujours le **m√™me vecteur num√©rique**, quel que soit le contexte. Or, dans de nombreuses langues, notamment en fran√ßais, **un m√™me mot peut avoir plusieurs significations** selon son usage. De plus, **des notions diff√©rentes peuvent s‚Äô√©crire de la m√™me fa√ßon** (homonymes).  

Pour d√©sambigu√Øser ces cas, il est essentiel de prendre en compte **le contexte** dans lequel un mot appara√Æt afin de d√©terminer son sens pr√©cis.  

Les **embeddings contextuels** permettent d‚Äôadapter la repr√©sentation num√©rique d‚Äôun mot **en fonction de son contexte**. Contrairement aux approches statiques, ils tiennent compte **de la position et de l'interaction entre les mots** dans une phrase.  

**Exemple :**  
- *"La **baleine** nage dans l‚Äôoc√©an."* üêã ‚Üí **baleine (animal)**  
- *"J‚Äôai un mal de **baleine** apr√®s ce repas."* ü§ï ‚Üí **baleine (expression signifiant une douleur intense)**  

Les mod√®les bas√©s sur les **transformers** (ex. **BERT, GPT, T5**) sont capables d‚Äôencoder ces diff√©rences en g√©n√©rant un **vecteur unique pour chaque occurrence d‚Äôun mot** selon son contexte. Ainsi, un mot aura un embedding sp√©cifique **qui varie en fonction des mots qui l'entourent**, tout en **conservant les relations s√©mantiques** apprises (synonymie, antonymie, etc.).

## Travaux pratiques